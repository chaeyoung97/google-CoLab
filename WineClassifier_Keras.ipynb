{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WineClassifier_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyoung97/google-CoLab/blob/master/WineClassifier_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3rsyN-78ocI6",
        "outputId": "7725bd8f-308b-491d-ce48-b1a94f794c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RdWzUjvZobV4",
        "outputId": "5f59d5d1-c6e2-4469-ffb8-2ebaf2e27774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ruFtS02AobWA",
        "colab": {}
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5kTZyX1obWD",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.__version__\n",
        "pd.options.display.max_rows=15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G0thM0X0obWG",
        "outputId": "8e704c7a-7423-413d-9221-3c50cb81e8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "np.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.18.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t0s-aG2SobWJ",
        "colab": {}
      },
      "source": [
        "#########################코드########################\n",
        "red_wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
        "white_wine = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')\n",
        "\n",
        "\n",
        "#####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IR2Bk48fobWM",
        "outputId": "6e65185f-7770-42dc-acea-7b2c1a3abef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "display(white_wine)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.00100</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.99400</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.99510</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.99560</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.99560</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4893</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.039</td>\n",
              "      <td>24.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.99114</td>\n",
              "      <td>3.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4894</th>\n",
              "      <td>6.6</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.36</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.047</td>\n",
              "      <td>57.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4895</th>\n",
              "      <td>6.5</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.041</td>\n",
              "      <td>30.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.99254</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4896</th>\n",
              "      <td>5.5</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.1</td>\n",
              "      <td>0.022</td>\n",
              "      <td>20.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>0.98869</td>\n",
              "      <td>3.34</td>\n",
              "      <td>0.38</td>\n",
              "      <td>12.8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4897</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.020</td>\n",
              "      <td>22.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.98941</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.32</td>\n",
              "      <td>11.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4898 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0               7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1               6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2               8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3               7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4               7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "...             ...               ...          ...  ...        ...      ...      ...\n",
              "4893            6.2              0.21         0.29  ...       0.50     11.2        6\n",
              "4894            6.6              0.32         0.36  ...       0.46      9.6        5\n",
              "4895            6.5              0.24         0.19  ...       0.46      9.4        6\n",
              "4896            5.5              0.29         0.30  ...       0.38     12.8        7\n",
              "4897            6.0              0.21         0.38  ...       0.32     11.8        6\n",
              "\n",
              "[4898 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aoIonV7KobWP",
        "outputId": "08d1e73b-3cdf-4952-a9e4-efc086ce3e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "display(red_wine)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "1               7.8             0.880         0.00  ...       0.68      9.8        5\n",
              "2               7.8             0.760         0.04  ...       0.65      9.8        5\n",
              "3              11.2             0.280         0.56  ...       0.58      9.8        6\n",
              "4               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "...             ...               ...          ...  ...        ...      ...      ...\n",
              "1594            6.2             0.600         0.08  ...       0.58     10.5        5\n",
              "1595            5.9             0.550         0.10  ...       0.76     11.2        6\n",
              "1596            6.3             0.510         0.13  ...       0.75     11.0        6\n",
              "1597            5.9             0.645         0.12  ...       0.71     10.2        5\n",
              "1598            6.0             0.310         0.47  ...       0.66     11.0        6\n",
              "\n",
              "[1599 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sHHRgjpWobWT",
        "colab": {}
      },
      "source": [
        "#####################################################\n",
        "from sklearn.model_selection import train_test_split\n",
        "def generate_data(df, t_r):\n",
        "\n",
        "    X = df.drop('quality', axis = 1) # axis 는 삭제를 행 /열 중 뭐로 기준할 것인지 명시함 1이면 열 기준 삭제\n",
        "    Y = df['quality']\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test  = train_test_split(X, Y, test_size = 1 - t_r, random_state = 64) #random_state하면 실행 시킬 때 마다 같은 기준으로 나눠줌!\n",
        "    Y_train, Y_test = Y_train.values, Y_test.values\n",
        "    return X_train.values, Y_train, X_test.values, Y_test\n",
        "#####################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A-kAXFUkobWV",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_test, y_test = generate_data(white_wine, 0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S5AcTqXirq_",
        "colab_type": "code",
        "outputId": "a07ee8a5-9c3f-4823-ddcf-a7fccb84f61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(\"x_train: \" ,x_train.shape, \"\\ny_train: \",y_train.shape, \"\\nx_test : \",x_test.shape,\"\\ny_test : \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train:  (3428, 11) \n",
            "y_train:  (3428,) \n",
            "x_test :  (1470, 11) \n",
            "y_test :  (1470,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FO4OxZuhobWZ"
      },
      "source": [
        "\n",
        "* 하나의 히든 레이어에 32개의 노드를 가진 인공신경망 모델 생성 및 모델 학습\n",
        "* 트레이닝 Epoch에 따라 Loss의 변화를 그래프로 시각화\n",
        "* 테스트 셋에 대한 정확도 기록"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gE8UhHrNobWZ",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "import tensorflow as tf\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=32,activation = 'relu', input_dim = 11)) #input_shape=(11,)\n",
        "model.add(Dense(units =11, activation = 'softmax'))\n",
        "\n",
        "model.compile(optimizer = 'Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "###########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGjpPNnSGIS3",
        "colab_type": "code",
        "outputId": "b04e3656-412a-4208-d6a3-db88d68c14c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 32)                384       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 747\n",
            "Trainable params: 747\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVokFBz7IEh7",
        "colab_type": "code",
        "outputId": "02f30d11-48d6-4fe3-e724-6e920352caa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs =200) #batch_size = 몇개의 샘플로 가중치 갱신할 것인지 지정"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "3428/3428 [==============================] - 0s 83us/step - loss: 4.8200 - accuracy: 0.4046\n",
            "Epoch 2/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.5690 - accuracy: 0.4064\n",
            "Epoch 3/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.3556 - accuracy: 0.4317\n",
            "Epoch 4/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.2957 - accuracy: 0.4373\n",
            "Epoch 5/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.2947 - accuracy: 0.4361\n",
            "Epoch 6/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.2663 - accuracy: 0.4533\n",
            "Epoch 7/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.2481 - accuracy: 0.4641\n",
            "Epoch 8/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.2495 - accuracy: 0.4577\n",
            "Epoch 9/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.2650 - accuracy: 0.4501\n",
            "Epoch 10/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.2482 - accuracy: 0.4501\n",
            "Epoch 11/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.2259 - accuracy: 0.4592\n",
            "Epoch 12/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.2148 - accuracy: 0.4711\n",
            "Epoch 13/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.2233 - accuracy: 0.4749\n",
            "Epoch 14/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.2253 - accuracy: 0.4665\n",
            "Epoch 15/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.2019 - accuracy: 0.4720\n",
            "Epoch 16/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.2219 - accuracy: 0.4737\n",
            "Epoch 17/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1977 - accuracy: 0.4775\n",
            "Epoch 18/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1931 - accuracy: 0.4705\n",
            "Epoch 19/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1892 - accuracy: 0.4819\n",
            "Epoch 20/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1882 - accuracy: 0.4755\n",
            "Epoch 21/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.2158 - accuracy: 0.4717\n",
            "Epoch 22/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1879 - accuracy: 0.4834\n",
            "Epoch 23/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.2009 - accuracy: 0.4702\n",
            "Epoch 24/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.2103 - accuracy: 0.4790\n",
            "Epoch 25/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.2023 - accuracy: 0.4810\n",
            "Epoch 26/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1952 - accuracy: 0.4860\n",
            "Epoch 27/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.2016 - accuracy: 0.4697\n",
            "Epoch 28/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1916 - accuracy: 0.4758\n",
            "Epoch 29/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1793 - accuracy: 0.4772\n",
            "Epoch 30/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1764 - accuracy: 0.4875\n",
            "Epoch 31/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1812 - accuracy: 0.4851\n",
            "Epoch 32/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1845 - accuracy: 0.4781\n",
            "Epoch 33/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1749 - accuracy: 0.4921\n",
            "Epoch 34/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1706 - accuracy: 0.4883\n",
            "Epoch 35/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1673 - accuracy: 0.4883\n",
            "Epoch 36/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1680 - accuracy: 0.4875\n",
            "Epoch 37/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1818 - accuracy: 0.5018\n",
            "Epoch 38/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1711 - accuracy: 0.4886\n",
            "Epoch 39/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1598 - accuracy: 0.4924\n",
            "Epoch 40/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1732 - accuracy: 0.4936\n",
            "Epoch 41/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1629 - accuracy: 0.4851\n",
            "Epoch 42/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1592 - accuracy: 0.4953\n",
            "Epoch 43/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1612 - accuracy: 0.4956\n",
            "Epoch 44/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1800 - accuracy: 0.4825\n",
            "Epoch 45/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1502 - accuracy: 0.5020\n",
            "Epoch 46/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1557 - accuracy: 0.4974\n",
            "Epoch 47/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1403 - accuracy: 0.4945\n",
            "Epoch 48/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1515 - accuracy: 0.5018\n",
            "Epoch 49/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1484 - accuracy: 0.4982\n",
            "Epoch 50/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1548 - accuracy: 0.4845\n",
            "Epoch 51/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.1651 - accuracy: 0.4924\n",
            "Epoch 52/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1452 - accuracy: 0.4988\n",
            "Epoch 53/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1424 - accuracy: 0.4956\n",
            "Epoch 54/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1531 - accuracy: 0.4959\n",
            "Epoch 55/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1417 - accuracy: 0.5111\n",
            "Epoch 56/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1589 - accuracy: 0.4982\n",
            "Epoch 57/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1453 - accuracy: 0.5041\n",
            "Epoch 58/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1444 - accuracy: 0.4991\n",
            "Epoch 59/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1522 - accuracy: 0.4924\n",
            "Epoch 60/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1514 - accuracy: 0.4977\n",
            "Epoch 61/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1346 - accuracy: 0.5064\n",
            "Epoch 62/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1384 - accuracy: 0.5105\n",
            "Epoch 63/200\n",
            "3428/3428 [==============================] - 0s 64us/step - loss: 1.1398 - accuracy: 0.4985\n",
            "Epoch 64/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1450 - accuracy: 0.4924\n",
            "Epoch 65/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1274 - accuracy: 0.5073\n",
            "Epoch 66/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1393 - accuracy: 0.4997\n",
            "Epoch 67/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1308 - accuracy: 0.5003\n",
            "Epoch 68/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1525 - accuracy: 0.4877\n",
            "Epoch 69/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1538 - accuracy: 0.4971\n",
            "Epoch 70/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1299 - accuracy: 0.5029\n",
            "Epoch 71/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1380 - accuracy: 0.4988\n",
            "Epoch 72/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1229 - accuracy: 0.5105\n",
            "Epoch 73/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1335 - accuracy: 0.4933\n",
            "Epoch 74/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1212 - accuracy: 0.5055\n",
            "Epoch 75/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1411 - accuracy: 0.4950\n",
            "Epoch 76/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1280 - accuracy: 0.5088\n",
            "Epoch 77/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1291 - accuracy: 0.5000\n",
            "Epoch 78/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1392 - accuracy: 0.5082\n",
            "Epoch 79/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1373 - accuracy: 0.5012\n",
            "Epoch 80/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1211 - accuracy: 0.5038\n",
            "Epoch 81/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1307 - accuracy: 0.5158\n",
            "Epoch 82/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1191 - accuracy: 0.5032\n",
            "Epoch 83/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.1286 - accuracy: 0.5050\n",
            "Epoch 84/200\n",
            "3428/3428 [==============================] - 0s 64us/step - loss: 1.1232 - accuracy: 0.5175\n",
            "Epoch 85/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1264 - accuracy: 0.5085\n",
            "Epoch 86/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1502 - accuracy: 0.4956\n",
            "Epoch 87/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1249 - accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1198 - accuracy: 0.5117\n",
            "Epoch 89/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1237 - accuracy: 0.4968\n",
            "Epoch 90/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1372 - accuracy: 0.5026\n",
            "Epoch 91/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1155 - accuracy: 0.5146\n",
            "Epoch 92/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1240 - accuracy: 0.5096\n",
            "Epoch 93/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1367 - accuracy: 0.4959\n",
            "Epoch 94/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1179 - accuracy: 0.5152\n",
            "Epoch 95/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1218 - accuracy: 0.5029\n",
            "Epoch 96/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1128 - accuracy: 0.5140\n",
            "Epoch 97/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1233 - accuracy: 0.5175\n",
            "Epoch 98/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1267 - accuracy: 0.5038\n",
            "Epoch 99/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1291 - accuracy: 0.5041\n",
            "Epoch 100/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1203 - accuracy: 0.5020\n",
            "Epoch 101/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1148 - accuracy: 0.5149\n",
            "Epoch 102/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1397 - accuracy: 0.4994\n",
            "Epoch 103/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1247 - accuracy: 0.5073\n",
            "Epoch 104/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1158 - accuracy: 0.5178\n",
            "Epoch 105/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1306 - accuracy: 0.5047\n",
            "Epoch 106/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1172 - accuracy: 0.5169\n",
            "Epoch 107/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1133 - accuracy: 0.5163\n",
            "Epoch 108/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1117 - accuracy: 0.5178\n",
            "Epoch 109/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1285 - accuracy: 0.5047\n",
            "Epoch 110/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1088 - accuracy: 0.5219\n",
            "Epoch 111/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1099 - accuracy: 0.5239\n",
            "Epoch 112/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1205 - accuracy: 0.5061\n",
            "Epoch 113/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1224 - accuracy: 0.5073\n",
            "Epoch 114/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1142 - accuracy: 0.5085\n",
            "Epoch 115/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1227 - accuracy: 0.5006\n",
            "Epoch 116/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1150 - accuracy: 0.5108\n",
            "Epoch 117/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1087 - accuracy: 0.5163\n",
            "Epoch 118/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1135 - accuracy: 0.5131\n",
            "Epoch 119/200\n",
            "3428/3428 [==============================] - 0s 64us/step - loss: 1.1020 - accuracy: 0.5128\n",
            "Epoch 120/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1102 - accuracy: 0.5088\n",
            "Epoch 121/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1084 - accuracy: 0.5216\n",
            "Epoch 122/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1183 - accuracy: 0.5055\n",
            "Epoch 123/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1128 - accuracy: 0.5134\n",
            "Epoch 124/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1093 - accuracy: 0.5230\n",
            "Epoch 125/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.0992 - accuracy: 0.5257\n",
            "Epoch 126/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1071 - accuracy: 0.5128\n",
            "Epoch 127/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1131 - accuracy: 0.5067\n",
            "Epoch 128/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1186 - accuracy: 0.5172\n",
            "Epoch 129/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1183 - accuracy: 0.5131\n",
            "Epoch 130/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1351 - accuracy: 0.5023\n",
            "Epoch 131/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1121 - accuracy: 0.5219\n",
            "Epoch 132/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1069 - accuracy: 0.5169\n",
            "Epoch 133/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.0989 - accuracy: 0.5300\n",
            "Epoch 134/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1235 - accuracy: 0.5053\n",
            "Epoch 135/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1005 - accuracy: 0.5198\n",
            "Epoch 136/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1227 - accuracy: 0.5149\n",
            "Epoch 137/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1135 - accuracy: 0.5079\n",
            "Epoch 138/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1117 - accuracy: 0.5175\n",
            "Epoch 139/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1034 - accuracy: 0.5239\n",
            "Epoch 140/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1073 - accuracy: 0.5163\n",
            "Epoch 141/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1116 - accuracy: 0.5280\n",
            "Epoch 142/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.1140 - accuracy: 0.5140\n",
            "Epoch 143/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1141 - accuracy: 0.5128\n",
            "Epoch 144/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1169 - accuracy: 0.4988\n",
            "Epoch 145/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1095 - accuracy: 0.5155\n",
            "Epoch 146/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1098 - accuracy: 0.5090\n",
            "Epoch 147/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1116 - accuracy: 0.5190\n",
            "Epoch 148/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1013 - accuracy: 0.5292\n",
            "Epoch 149/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1011 - accuracy: 0.5169\n",
            "Epoch 150/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1184 - accuracy: 0.5120\n",
            "Epoch 151/200\n",
            "3428/3428 [==============================] - 0s 65us/step - loss: 1.1058 - accuracy: 0.5260\n",
            "Epoch 152/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.1049 - accuracy: 0.5260\n",
            "Epoch 153/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1092 - accuracy: 0.5274\n",
            "Epoch 154/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.1263 - accuracy: 0.5172\n",
            "Epoch 155/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1164 - accuracy: 0.5105\n",
            "Epoch 156/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.1058 - accuracy: 0.5225\n",
            "Epoch 157/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1111 - accuracy: 0.5058\n",
            "Epoch 158/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1019 - accuracy: 0.5172\n",
            "Epoch 159/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.0950 - accuracy: 0.5207\n",
            "Epoch 160/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.1005 - accuracy: 0.5230\n",
            "Epoch 161/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.1109 - accuracy: 0.5047\n",
            "Epoch 162/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.0940 - accuracy: 0.5242\n",
            "Epoch 163/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1035 - accuracy: 0.5146\n",
            "Epoch 164/200\n",
            "3428/3428 [==============================] - 0s 75us/step - loss: 1.1094 - accuracy: 0.5111\n",
            "Epoch 165/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1068 - accuracy: 0.5067\n",
            "Epoch 166/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.1172 - accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1064 - accuracy: 0.5213\n",
            "Epoch 168/200\n",
            "3428/3428 [==============================] - 0s 75us/step - loss: 1.1044 - accuracy: 0.5219\n",
            "Epoch 169/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.0954 - accuracy: 0.5228\n",
            "Epoch 170/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.0943 - accuracy: 0.5204\n",
            "Epoch 171/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.0946 - accuracy: 0.5283\n",
            "Epoch 172/200\n",
            "3428/3428 [==============================] - 0s 75us/step - loss: 1.0971 - accuracy: 0.5225\n",
            "Epoch 173/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1087 - accuracy: 0.5143\n",
            "Epoch 174/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.0999 - accuracy: 0.5131\n",
            "Epoch 175/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.0999 - accuracy: 0.5207\n",
            "Epoch 176/200\n",
            "3428/3428 [==============================] - 0s 76us/step - loss: 1.1095 - accuracy: 0.5125\n",
            "Epoch 177/200\n",
            "3428/3428 [==============================] - 0s 79us/step - loss: 1.1011 - accuracy: 0.5158\n",
            "Epoch 178/200\n",
            "3428/3428 [==============================] - 0s 78us/step - loss: 1.1110 - accuracy: 0.5073\n",
            "Epoch 179/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1097 - accuracy: 0.5190\n",
            "Epoch 180/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.1003 - accuracy: 0.5146\n",
            "Epoch 181/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.1203 - accuracy: 0.5044\n",
            "Epoch 182/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.0973 - accuracy: 0.5225\n",
            "Epoch 183/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.1028 - accuracy: 0.5166\n",
            "Epoch 184/200\n",
            "3428/3428 [==============================] - 0s 77us/step - loss: 1.1099 - accuracy: 0.5044\n",
            "Epoch 185/200\n",
            "3428/3428 [==============================] - 0s 76us/step - loss: 1.1058 - accuracy: 0.5102\n",
            "Epoch 186/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.1110 - accuracy: 0.5140\n",
            "Epoch 187/200\n",
            "3428/3428 [==============================] - 0s 71us/step - loss: 1.1031 - accuracy: 0.5210\n",
            "Epoch 188/200\n",
            "3428/3428 [==============================] - 0s 75us/step - loss: 1.0979 - accuracy: 0.5207\n",
            "Epoch 189/200\n",
            "3428/3428 [==============================] - 0s 75us/step - loss: 1.1000 - accuracy: 0.5050\n",
            "Epoch 190/200\n",
            "3428/3428 [==============================] - 0s 72us/step - loss: 1.0968 - accuracy: 0.5143\n",
            "Epoch 191/200\n",
            "3428/3428 [==============================] - 0s 73us/step - loss: 1.1097 - accuracy: 0.5128\n",
            "Epoch 192/200\n",
            "3428/3428 [==============================] - 0s 74us/step - loss: 1.0880 - accuracy: 0.5283\n",
            "Epoch 193/200\n",
            "3428/3428 [==============================] - 0s 79us/step - loss: 1.0966 - accuracy: 0.5216\n",
            "Epoch 194/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1081 - accuracy: 0.5198\n",
            "Epoch 195/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1128 - accuracy: 0.5125\n",
            "Epoch 196/200\n",
            "3428/3428 [==============================] - 0s 69us/step - loss: 1.1029 - accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "3428/3428 [==============================] - 0s 70us/step - loss: 1.1084 - accuracy: 0.5041\n",
            "Epoch 198/200\n",
            "3428/3428 [==============================] - 0s 68us/step - loss: 1.1115 - accuracy: 0.5125\n",
            "Epoch 199/200\n",
            "3428/3428 [==============================] - 0s 67us/step - loss: 1.1072 - accuracy: 0.5131\n",
            "Epoch 200/200\n",
            "3428/3428 [==============================] - 0s 66us/step - loss: 1.1064 - accuracy: 0.5187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgLv92sdVCsV",
        "colab_type": "code",
        "outputId": "5bebc3b4-e33e-47e8-8ce6-93d38de66fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "####################################\n",
        "#트레이닝 epoch에 따라 loss와 accuracy 의 변화를 그래프로 시각화\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig , loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(history.history['loss'], 'r', label = 'train loss')\n",
        "\n",
        "acc_ax.plot(history.history['accuracy'], 'b', label = 'train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='lower left')\n",
        "acc_ax.legend(loc = 'upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSHMe6wkYmaP",
        "colab_type": "code",
        "outputId": "e0224947-7e9a-4ea8-8b3f-23cd2dec6a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"test accuracy: \",test_accuracy,\"\\ntest loss:    \",test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1470/1470 [==============================] - 0s 48us/step\n",
            "test accuracy:  0.5292516946792603 \n",
            "test loss:     1.1421082686404793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOBsaGyndyV",
        "colab_type": "text"
      },
      "source": [
        "<앞에서 만든 모델 성능을 향상시킬 방법을 적용>\n",
        "1. 하이퍼파라미터 변경하여  테스트 셋 정확도 향상 \n",
        "(레이어 수 , 노드 수, 러닝 레이트 등..)\n",
        "2. 하이퍼파라미터를 변화시킨 각각의 모델에 대해 , 트레이닝 epoch당 loss변활를 기록, 시각화\n",
        "3. 그 외에 여러 방법 적용하여 가장 성능 좋은 모델을 선택함 dropout nomalization 등.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5JOT7FBobWc",
        "outputId": "8e14a1da-6552-4566-e5b6-99f664c4562f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "##########################################################\n",
        "\n",
        "##1 Normalization (데이터를 0~1로 normalize한다.)\n",
        "\n",
        "\n",
        "\n",
        "def norm(w):\n",
        "    wine = w.copy()\n",
        "    for i in wine:\n",
        "        if(i=='quality'):\n",
        "            continue\n",
        "        wine[i] = (wine[i] - wine[i].min()) / (wine[i].max() - wine[i].min())\n",
        "    return wine\n",
        "\n",
        "red1 = norm(red_wine)\n",
        "white1 = norm(white_wine)\n",
        "print(red1.describe())\n",
        "print(white1.describe())\n",
        "print(white_wine.describe())\n",
        "\n",
        "\n",
        "###########################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    1599.000000       1599.000000  ...  1599.000000  1599.000000\n",
            "mean        0.329171          0.279329  ...     0.311228     5.636023\n",
            "std         0.154079          0.122644  ...     0.163949     0.807569\n",
            "min         0.000000          0.000000  ...     0.000000     3.000000\n",
            "25%         0.221239          0.184932  ...     0.169231     5.000000\n",
            "50%         0.292035          0.273973  ...     0.276923     6.000000\n",
            "75%         0.407080          0.356164  ...     0.415385     6.000000\n",
            "max         1.000000          1.000000  ...     1.000000     8.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        0.293730          0.194354  ...     0.405527     5.877909\n",
            "std         0.081141          0.098818  ...     0.198487     0.885639\n",
            "min         0.000000          0.000000  ...     0.000000     3.000000\n",
            "25%         0.240385          0.127451  ...     0.241935     5.000000\n",
            "50%         0.288462          0.176471  ...     0.387097     6.000000\n",
            "75%         0.336538          0.235294  ...     0.548387     6.000000\n",
            "max         1.000000          1.000000  ...     1.000000     9.000000\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "       fixed acidity  volatile acidity  ...      alcohol      quality\n",
            "count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n",
            "mean        6.854788          0.278241  ...    10.514267     5.877909\n",
            "std         0.843868          0.100795  ...     1.230621     0.885639\n",
            "min         3.800000          0.080000  ...     8.000000     3.000000\n",
            "25%         6.300000          0.210000  ...     9.500000     5.000000\n",
            "50%         6.800000          0.260000  ...    10.400000     6.000000\n",
            "75%         7.300000          0.320000  ...    11.400000     6.000000\n",
            "max        14.200000          1.100000  ...    14.200000     9.000000\n",
            "\n",
            "[8 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wm2RS6euGnN",
        "colab_type": "code",
        "outputId": "484b930b-2070-47e0-f8ad-23edc930989f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "##2. model의 하이퍼파라미터 변경\n",
        "from keras.layers import Dropout ,BatchNormalization\n",
        "from keras.optimizers import Adam , SGD\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(units=2048,activation = 'relu', input_dim =11)) #input_shape=(11,)\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dense(units =2048, activation = 'relu'))\n",
        "\n",
        "model2.add(Dense(units =1024, activation = 'relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(units =512, activation = 'relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dense(units =512, activation = 'relu'))\n",
        "model2.add(Dense(units =256, activation = 'relu'))\n",
        "model2.add(Dense(units =128, activation = 'relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(Dense(units =64, activation = 'relu'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dense(units =32, activation = 'relu'))\n",
        "model2.add(Dense(units=11, activation='softmax'))\n",
        "#목표인 퀄리티가 정수여서 sparse_categorical_crossentropy사용함\n",
        "#sgd = SGD(lr = 0.01 , decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "model2.compile(optimizer = Adam(lr = 0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 2048)              24576     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 2048)              8192      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2048)              4196352   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 7,291,979\n",
            "Trainable params: 7,286,731\n",
            "Non-trainable params: 5,248\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G51jzp3NuRCA",
        "colab_type": "code",
        "outputId": "66f6822b-be46-4667-b0a6-85251a7090f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "x_train2, y_train2, x_test2, y_test2 = generate_data(white1, 0.7)\n",
        "\n",
        "history = model2.fit(x_train2, y_train2, epochs =200) #batch_size = 몇개의 샘플로 가중치 갱신할 것인지 지정\n",
        "# validation_data =(x_val, y_val), callbacks = [early_stopping]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "3428/3428 [==============================] - 1s 259us/step - loss: 1.3706 - accuracy: 0.4673\n",
            "Epoch 2/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 1.1274 - accuracy: 0.5213\n",
            "Epoch 3/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 1.0953 - accuracy: 0.5207\n",
            "Epoch 4/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 1.0586 - accuracy: 0.5432\n",
            "Epoch 5/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 1.0500 - accuracy: 0.5470\n",
            "Epoch 6/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 1.0366 - accuracy: 0.5621\n",
            "Epoch 7/200\n",
            "3428/3428 [==============================] - 1s 261us/step - loss: 1.0259 - accuracy: 0.5581\n",
            "Epoch 8/200\n",
            "3428/3428 [==============================] - 1s 262us/step - loss: 1.0124 - accuracy: 0.5729\n",
            "Epoch 9/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.9976 - accuracy: 0.5773\n",
            "Epoch 10/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.9809 - accuracy: 0.5837\n",
            "Epoch 11/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.9771 - accuracy: 0.5761\n",
            "Epoch 12/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.9702 - accuracy: 0.5898\n",
            "Epoch 13/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.9612 - accuracy: 0.5875\n",
            "Epoch 14/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.9404 - accuracy: 0.5898\n",
            "Epoch 15/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.9372 - accuracy: 0.6074\n",
            "Epoch 16/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.9323 - accuracy: 0.6033\n",
            "Epoch 17/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.9080 - accuracy: 0.6155\n",
            "Epoch 18/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.9104 - accuracy: 0.6018\n",
            "Epoch 19/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.8919 - accuracy: 0.6237\n",
            "Epoch 20/200\n",
            "3428/3428 [==============================] - 1s 264us/step - loss: 0.8789 - accuracy: 0.6278\n",
            "Epoch 21/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.8939 - accuracy: 0.6225\n",
            "Epoch 22/200\n",
            "3428/3428 [==============================] - 1s 262us/step - loss: 0.8605 - accuracy: 0.6263\n",
            "Epoch 23/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.8403 - accuracy: 0.6564\n",
            "Epoch 24/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.8183 - accuracy: 0.6464\n",
            "Epoch 25/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.8330 - accuracy: 0.6485\n",
            "Epoch 26/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.8028 - accuracy: 0.6654\n",
            "Epoch 27/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.7974 - accuracy: 0.6634\n",
            "Epoch 28/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.7630 - accuracy: 0.6794\n",
            "Epoch 29/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.7499 - accuracy: 0.6949\n",
            "Epoch 30/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.8005 - accuracy: 0.6654\n",
            "Epoch 31/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.8270 - accuracy: 0.6529\n",
            "Epoch 32/200\n",
            "3428/3428 [==============================] - 1s 261us/step - loss: 0.8080 - accuracy: 0.6631\n",
            "Epoch 33/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.7480 - accuracy: 0.6861\n",
            "Epoch 34/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.7137 - accuracy: 0.7007\n",
            "Epoch 35/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.7371 - accuracy: 0.6928\n",
            "Epoch 36/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.6872 - accuracy: 0.7237\n",
            "Epoch 37/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.6760 - accuracy: 0.7249\n",
            "Epoch 38/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.6356 - accuracy: 0.7395\n",
            "Epoch 39/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.7093 - accuracy: 0.7100\n",
            "Epoch 40/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.6615 - accuracy: 0.7202\n",
            "Epoch 41/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.6316 - accuracy: 0.7447\n",
            "Epoch 42/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.6373 - accuracy: 0.7401\n",
            "Epoch 43/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.6064 - accuracy: 0.7550\n",
            "Epoch 44/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.6297 - accuracy: 0.7500\n",
            "Epoch 45/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.6219 - accuracy: 0.7480\n",
            "Epoch 46/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.5770 - accuracy: 0.7666\n",
            "Epoch 47/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.5663 - accuracy: 0.7701\n",
            "Epoch 48/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.5215 - accuracy: 0.7862\n",
            "Epoch 49/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.5516 - accuracy: 0.7690\n",
            "Epoch 50/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.5052 - accuracy: 0.7932\n",
            "Epoch 51/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.5424 - accuracy: 0.7830\n",
            "Epoch 52/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.4941 - accuracy: 0.8116\n",
            "Epoch 53/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.4807 - accuracy: 0.8013\n",
            "Epoch 54/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.4760 - accuracy: 0.8148\n",
            "Epoch 55/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.4874 - accuracy: 0.8095\n",
            "Epoch 56/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.5300 - accuracy: 0.7935\n",
            "Epoch 57/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.4375 - accuracy: 0.8261\n",
            "Epoch 58/200\n",
            "3428/3428 [==============================] - 1s 259us/step - loss: 0.4483 - accuracy: 0.8223\n",
            "Epoch 59/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.4220 - accuracy: 0.8363\n",
            "Epoch 60/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.3933 - accuracy: 0.8483\n",
            "Epoch 61/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.4217 - accuracy: 0.8355\n",
            "Epoch 62/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.4144 - accuracy: 0.8466\n",
            "Epoch 63/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.3934 - accuracy: 0.8431\n",
            "Epoch 64/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.3743 - accuracy: 0.8504\n",
            "Epoch 65/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.3807 - accuracy: 0.8515\n",
            "Epoch 66/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.3677 - accuracy: 0.8611\n",
            "Epoch 67/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.3716 - accuracy: 0.8559\n",
            "Epoch 68/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.3770 - accuracy: 0.8512\n",
            "Epoch 69/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.3890 - accuracy: 0.8515\n",
            "Epoch 70/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.3226 - accuracy: 0.8810\n",
            "Epoch 71/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.3776 - accuracy: 0.8504\n",
            "Epoch 72/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.3472 - accuracy: 0.8617\n",
            "Epoch 73/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.3067 - accuracy: 0.8821\n",
            "Epoch 74/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.3677 - accuracy: 0.8588\n",
            "Epoch 75/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.3465 - accuracy: 0.8655\n",
            "Epoch 76/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.3118 - accuracy: 0.8810\n",
            "Epoch 77/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.3886 - accuracy: 0.8553\n",
            "Epoch 78/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.3317 - accuracy: 0.8743\n",
            "Epoch 79/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.4091 - accuracy: 0.8393\n",
            "Epoch 80/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.3796 - accuracy: 0.8506\n",
            "Epoch 81/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.3611 - accuracy: 0.8632\n",
            "Epoch 82/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.3132 - accuracy: 0.8813\n",
            "Epoch 83/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.3366 - accuracy: 0.8664\n",
            "Epoch 84/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2873 - accuracy: 0.8915\n",
            "Epoch 85/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.2713 - accuracy: 0.8982\n",
            "Epoch 86/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.3014 - accuracy: 0.8810\n",
            "Epoch 87/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2839 - accuracy: 0.8976\n",
            "Epoch 88/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.2673 - accuracy: 0.9023\n",
            "Epoch 89/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.2856 - accuracy: 0.8918\n",
            "Epoch 90/200\n",
            "3428/3428 [==============================] - 1s 249us/step - loss: 0.2651 - accuracy: 0.8988\n",
            "Epoch 91/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2750 - accuracy: 0.8956\n",
            "Epoch 92/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2404 - accuracy: 0.9037\n",
            "Epoch 93/200\n",
            "3428/3428 [==============================] - 1s 264us/step - loss: 0.2504 - accuracy: 0.9087\n",
            "Epoch 94/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.3218 - accuracy: 0.8766\n",
            "Epoch 95/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.2584 - accuracy: 0.9023\n",
            "Epoch 96/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.2604 - accuracy: 0.8996\n",
            "Epoch 97/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.2684 - accuracy: 0.9017\n",
            "Epoch 98/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.2299 - accuracy: 0.9116\n",
            "Epoch 99/200\n",
            "3428/3428 [==============================] - 1s 259us/step - loss: 0.2695 - accuracy: 0.8991\n",
            "Epoch 100/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.2336 - accuracy: 0.9104\n",
            "Epoch 101/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.2156 - accuracy: 0.9201\n",
            "Epoch 102/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2530 - accuracy: 0.9052\n",
            "Epoch 103/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.3617 - accuracy: 0.8635\n",
            "Epoch 104/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2463 - accuracy: 0.9087\n",
            "Epoch 105/200\n",
            "3428/3428 [==============================] - 1s 263us/step - loss: 0.2640 - accuracy: 0.9014\n",
            "Epoch 106/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.2274 - accuracy: 0.9177\n",
            "Epoch 107/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.2362 - accuracy: 0.9084\n",
            "Epoch 108/200\n",
            "3428/3428 [==============================] - 1s 260us/step - loss: 0.4113 - accuracy: 0.8448\n",
            "Epoch 109/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2641 - accuracy: 0.9026\n",
            "Epoch 110/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.2634 - accuracy: 0.9011\n",
            "Epoch 111/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.2358 - accuracy: 0.9110\n",
            "Epoch 112/200\n",
            "3428/3428 [==============================] - 1s 249us/step - loss: 0.2267 - accuracy: 0.9116\n",
            "Epoch 113/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.3671 - accuracy: 0.8623\n",
            "Epoch 114/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.2598 - accuracy: 0.9052\n",
            "Epoch 115/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1928 - accuracy: 0.9291\n",
            "Epoch 116/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.2120 - accuracy: 0.9183\n",
            "Epoch 117/200\n",
            "3428/3428 [==============================] - 1s 259us/step - loss: 0.2189 - accuracy: 0.9212\n",
            "Epoch 118/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1928 - accuracy: 0.9312\n",
            "Epoch 119/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.2137 - accuracy: 0.9233\n",
            "Epoch 120/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.2149 - accuracy: 0.9198\n",
            "Epoch 121/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2404 - accuracy: 0.9139\n",
            "Epoch 122/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1973 - accuracy: 0.9259\n",
            "Epoch 123/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1786 - accuracy: 0.9326\n",
            "Epoch 124/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2051 - accuracy: 0.9207\n",
            "Epoch 125/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.2166 - accuracy: 0.9169\n",
            "Epoch 126/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.2125 - accuracy: 0.9177\n",
            "Epoch 127/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.2138 - accuracy: 0.9207\n",
            "Epoch 128/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1849 - accuracy: 0.9285\n",
            "Epoch 129/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1757 - accuracy: 0.9320\n",
            "Epoch 130/200\n",
            "3428/3428 [==============================] - 1s 249us/step - loss: 0.2755 - accuracy: 0.9055\n",
            "Epoch 131/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2204 - accuracy: 0.9160\n",
            "Epoch 132/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.1976 - accuracy: 0.9256\n",
            "Epoch 133/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2038 - accuracy: 0.9256\n",
            "Epoch 134/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1855 - accuracy: 0.9352\n",
            "Epoch 135/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1849 - accuracy: 0.9317\n",
            "Epoch 136/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1648 - accuracy: 0.9399\n",
            "Epoch 137/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1985 - accuracy: 0.9253\n",
            "Epoch 138/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.2422 - accuracy: 0.9093\n",
            "Epoch 139/200\n",
            "3428/3428 [==============================] - 1s 247us/step - loss: 0.2064 - accuracy: 0.9262\n",
            "Epoch 140/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.1587 - accuracy: 0.9437\n",
            "Epoch 141/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1769 - accuracy: 0.9379\n",
            "Epoch 142/200\n",
            "3428/3428 [==============================] - 1s 249us/step - loss: 0.1576 - accuracy: 0.9452\n",
            "Epoch 143/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.2308 - accuracy: 0.9201\n",
            "Epoch 144/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1821 - accuracy: 0.9361\n",
            "Epoch 145/200\n",
            "3428/3428 [==============================] - 1s 247us/step - loss: 0.1790 - accuracy: 0.9335\n",
            "Epoch 146/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2423 - accuracy: 0.9166\n",
            "Epoch 147/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1871 - accuracy: 0.9344\n",
            "Epoch 148/200\n",
            "3428/3428 [==============================] - 1s 248us/step - loss: 0.2378 - accuracy: 0.9125\n",
            "Epoch 149/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.1924 - accuracy: 0.9317\n",
            "Epoch 150/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1856 - accuracy: 0.9329\n",
            "Epoch 151/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2220 - accuracy: 0.9160\n",
            "Epoch 152/200\n",
            "3428/3428 [==============================] - 1s 247us/step - loss: 0.1786 - accuracy: 0.9355\n",
            "Epoch 153/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1867 - accuracy: 0.9320\n",
            "Epoch 154/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1711 - accuracy: 0.9370\n",
            "Epoch 155/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1298 - accuracy: 0.9530\n",
            "Epoch 156/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.1682 - accuracy: 0.9393\n",
            "Epoch 157/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1840 - accuracy: 0.9384\n",
            "Epoch 158/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1484 - accuracy: 0.9487\n",
            "Epoch 159/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.1585 - accuracy: 0.9425\n",
            "Epoch 160/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.1817 - accuracy: 0.9361\n",
            "Epoch 161/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1662 - accuracy: 0.9408\n",
            "Epoch 162/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.2706 - accuracy: 0.9029\n",
            "Epoch 163/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1920 - accuracy: 0.9259\n",
            "Epoch 164/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1867 - accuracy: 0.9323\n",
            "Epoch 165/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1576 - accuracy: 0.9428\n",
            "Epoch 166/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.2177 - accuracy: 0.9198\n",
            "Epoch 167/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1429 - accuracy: 0.9498\n",
            "Epoch 168/200\n",
            "3428/3428 [==============================] - 1s 254us/step - loss: 0.1738 - accuracy: 0.9367\n",
            "Epoch 169/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1365 - accuracy: 0.9498\n",
            "Epoch 170/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.1446 - accuracy: 0.9425\n",
            "Epoch 171/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1625 - accuracy: 0.9422\n",
            "Epoch 172/200\n",
            "3428/3428 [==============================] - 1s 248us/step - loss: 0.1609 - accuracy: 0.9443\n",
            "Epoch 173/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1736 - accuracy: 0.9370\n",
            "Epoch 174/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.1411 - accuracy: 0.9487\n",
            "Epoch 175/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1428 - accuracy: 0.9457\n",
            "Epoch 176/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1341 - accuracy: 0.9510\n",
            "Epoch 177/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1985 - accuracy: 0.9265\n",
            "Epoch 178/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.3502 - accuracy: 0.8859\n",
            "Epoch 179/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.2258 - accuracy: 0.9204\n",
            "Epoch 180/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1816 - accuracy: 0.9332\n",
            "Epoch 181/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1491 - accuracy: 0.9463\n",
            "Epoch 182/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1714 - accuracy: 0.9361\n",
            "Epoch 183/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1537 - accuracy: 0.9469\n",
            "Epoch 184/200\n",
            "3428/3428 [==============================] - 1s 251us/step - loss: 0.1622 - accuracy: 0.9405\n",
            "Epoch 185/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.1618 - accuracy: 0.9417\n",
            "Epoch 186/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.1346 - accuracy: 0.9522\n",
            "Epoch 187/200\n",
            "3428/3428 [==============================] - 1s 261us/step - loss: 0.1472 - accuracy: 0.9452\n",
            "Epoch 188/200\n",
            "3428/3428 [==============================] - 1s 253us/step - loss: 0.2031 - accuracy: 0.9300\n",
            "Epoch 189/200\n",
            "3428/3428 [==============================] - 1s 258us/step - loss: 0.1646 - accuracy: 0.9405\n",
            "Epoch 190/200\n",
            "3428/3428 [==============================] - 1s 262us/step - loss: 0.1227 - accuracy: 0.9527\n",
            "Epoch 191/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1712 - accuracy: 0.9405\n",
            "Epoch 192/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1472 - accuracy: 0.9463\n",
            "Epoch 193/200\n",
            "3428/3428 [==============================] - 1s 257us/step - loss: 0.1293 - accuracy: 0.9530\n",
            "Epoch 194/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1328 - accuracy: 0.9487\n",
            "Epoch 195/200\n",
            "3428/3428 [==============================] - 1s 252us/step - loss: 0.1466 - accuracy: 0.9463\n",
            "Epoch 196/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.1430 - accuracy: 0.9522\n",
            "Epoch 197/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.2348 - accuracy: 0.9204\n",
            "Epoch 198/200\n",
            "3428/3428 [==============================] - 1s 256us/step - loss: 0.1591 - accuracy: 0.9431\n",
            "Epoch 199/200\n",
            "3428/3428 [==============================] - 1s 250us/step - loss: 0.1642 - accuracy: 0.9367\n",
            "Epoch 200/200\n",
            "3428/3428 [==============================] - 1s 255us/step - loss: 0.1676 - accuracy: 0.9387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV2o6DxW0Cqg",
        "colab_type": "code",
        "outputId": "3e1207a3-7b67-4fe5-b458-d110df054797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "####################################\n",
        "#트레이닝 epoch에 따라 loss와 accuracy 의 변화를 그래프로 시각화\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig , loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(history.history['loss'], 'r', label = 'train loss')\n",
        "\n",
        "acc_ax.plot(history.history['accuracy'], 'b', label = 'train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='lower left')\n",
        "acc_ax.legend(loc = 'upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF-S4lyI0Kui",
        "colab_type": "code",
        "outputId": "babb9964-c8c5-49b5-b558-f839a2f21e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_accuracy = model2.evaluate(x_test2, y_test2)\n",
        "print(\"test accuracy: \",test_accuracy,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1470/1470 [==============================] - 0s 68us/step\n",
            "test accuracy:  0.6319727897644043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KJ_KUtH2obWf"
      },
      "source": [
        "* 화이트 와인과 레드 와인 데이터를 합쳐 wine 데이터 셋 생성\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VWzy7FV8obWg",
        "outputId": "6d859449-6ac8-41ce-d254-13311b4d495a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "source": [
        "##########################################################\n",
        "#1. 데이터 합치기\n",
        "red2 = norm(red_wine)\n",
        "white2 = norm(white_wine)\n",
        "red2['type'] = 0.0\n",
        "white2['type'] = 1.0\n",
        "print(red2.head(2))\n",
        "print(white2.head(2))\n",
        "\n",
        "wine = pd.concat([red2, white2])\n",
        "x_train3, y_train3, x_test3, y_test3 = generate_data(wine, 0.7)\n",
        "\n",
        "\n",
        "model3= Sequential()\n",
        "model3.add(Dense(units=4096,activation = 'relu', input_dim =12)) #input_shape=(12,)\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dense(units =2048, activation = 'relu'))\n",
        "model3.add(Dense(units =1024, activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dense(units =512, activation = 'relu'))\n",
        "model3.add(Dense(units =512, activation = 'relu'))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(Dense(units =256, activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dense(units =128, activation = 'relu'))\n",
        "model3.add(Dropout(0.3))\n",
        "model3.add(Dense(units =64, activation = 'relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dense(units =32, activation = 'relu'))\n",
        "model3.add(Dense(units=11, activation='softmax'))\n",
        "#목표인 퀄리티가 정수여서 sparse_categorical_crossentropy사용함\n",
        "sgd = SGD(lr = 0.01 , decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "model3.compile(optimizer = Adam(lr = 0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.summary()\n",
        "pass\n",
        "\n",
        "\n",
        "###########################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   fixed acidity  volatile acidity  citric acid  ...   alcohol  quality  type\n",
            "0       0.247788          0.397260          0.0  ...  0.153846        5   0.0\n",
            "1       0.283186          0.520548          0.0  ...  0.215385        5   0.0\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "   fixed acidity  volatile acidity  citric acid  ...   alcohol  quality  type\n",
            "0       0.307692          0.186275     0.216867  ...  0.129032        6   1.0\n",
            "1       0.240385          0.215686     0.204819  ...  0.241935        6   1.0\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 4096)              53248     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 2048)              8390656   \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 11,526,219\n",
            "Trainable params: 11,515,339\n",
            "Non-trainable params: 10,880\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn8te1Jso5ZN",
        "colab_type": "code",
        "outputId": "6297e0d2-eec9-48dc-a83a-d1d7484c7f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "x_val, y_val ,x,y = generate_data(wine, 0.3)\n",
        "\n",
        "early_stopping = EarlyStopping(patience =50)\n",
        "history = model3.fit(x_train3, y_train3, epochs =2000, validation_data =(x_val, y_val), callbacks = [early_stopping]) #batch_size = 몇개의 샘플로 가중치 갱신할 것인지 지정\n",
        "# validation_data =(x_val, y_val), callbacks = [early_stopping]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4547 samples, validate on 1949 samples\n",
            "Epoch 1/2000\n",
            "4547/4547 [==============================] - 2s 501us/step - loss: 1.3930 - accuracy: 0.4614 - val_loss: 1.3194 - val_accuracy: 0.4844\n",
            "Epoch 2/2000\n",
            "4547/4547 [==============================] - 2s 357us/step - loss: 1.1676 - accuracy: 0.5107 - val_loss: 1.2141 - val_accuracy: 0.4341\n",
            "Epoch 3/2000\n",
            "4547/4547 [==============================] - 2s 352us/step - loss: 1.1345 - accuracy: 0.5140 - val_loss: 1.2234 - val_accuracy: 0.4767\n",
            "Epoch 4/2000\n",
            "4547/4547 [==============================] - 2s 350us/step - loss: 1.1005 - accuracy: 0.5291 - val_loss: 1.0905 - val_accuracy: 0.5105\n",
            "Epoch 5/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 1.0819 - accuracy: 0.5399 - val_loss: 1.0648 - val_accuracy: 0.5562\n",
            "Epoch 6/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0816 - accuracy: 0.5307 - val_loss: 1.0794 - val_accuracy: 0.5326\n",
            "Epoch 7/2000\n",
            "4547/4547 [==============================] - 2s 348us/step - loss: 1.0761 - accuracy: 0.5331 - val_loss: 1.0790 - val_accuracy: 0.5382\n",
            "Epoch 8/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 1.0770 - accuracy: 0.5373 - val_loss: 1.0400 - val_accuracy: 0.5582\n",
            "Epoch 9/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0652 - accuracy: 0.5445 - val_loss: 1.0398 - val_accuracy: 0.5536\n",
            "Epoch 10/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 1.0522 - accuracy: 0.5476 - val_loss: 1.0804 - val_accuracy: 0.5059\n",
            "Epoch 11/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 1.0548 - accuracy: 0.5569 - val_loss: 1.0709 - val_accuracy: 0.5331\n",
            "Epoch 12/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.1149 - accuracy: 0.5250 - val_loss: 1.0534 - val_accuracy: 0.5434\n",
            "Epoch 13/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 1.0718 - accuracy: 0.5296 - val_loss: 1.0712 - val_accuracy: 0.5249\n",
            "Epoch 14/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0872 - accuracy: 0.5285 - val_loss: 1.0732 - val_accuracy: 0.5357\n",
            "Epoch 15/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 1.0652 - accuracy: 0.5355 - val_loss: 1.0681 - val_accuracy: 0.5310\n",
            "Epoch 16/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 1.0512 - accuracy: 0.5558 - val_loss: 1.0248 - val_accuracy: 0.5475\n",
            "Epoch 17/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 1.0392 - accuracy: 0.5531 - val_loss: 1.0656 - val_accuracy: 0.5449\n",
            "Epoch 18/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 1.0524 - accuracy: 0.5467 - val_loss: 1.0320 - val_accuracy: 0.5562\n",
            "Epoch 19/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0462 - accuracy: 0.5498 - val_loss: 1.0246 - val_accuracy: 0.5598\n",
            "Epoch 20/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 1.0359 - accuracy: 0.5564 - val_loss: 1.0144 - val_accuracy: 0.5521\n",
            "Epoch 21/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 1.0296 - accuracy: 0.5516 - val_loss: 1.0053 - val_accuracy: 0.5562\n",
            "Epoch 22/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 1.0332 - accuracy: 0.5615 - val_loss: 1.0294 - val_accuracy: 0.5608\n",
            "Epoch 23/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 1.0249 - accuracy: 0.5670 - val_loss: 1.0331 - val_accuracy: 0.5511\n",
            "Epoch 24/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 1.0347 - accuracy: 0.5608 - val_loss: 1.0037 - val_accuracy: 0.5664\n",
            "Epoch 25/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 1.0222 - accuracy: 0.5586 - val_loss: 1.0217 - val_accuracy: 0.5654\n",
            "Epoch 26/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 1.0216 - accuracy: 0.5601 - val_loss: 1.0050 - val_accuracy: 0.5670\n",
            "Epoch 27/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 1.0181 - accuracy: 0.5707 - val_loss: 1.0006 - val_accuracy: 0.5752\n",
            "Epoch 28/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 1.0141 - accuracy: 0.5610 - val_loss: 0.9947 - val_accuracy: 0.5685\n",
            "Epoch 29/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 1.0129 - accuracy: 0.5641 - val_loss: 1.0087 - val_accuracy: 0.5834\n",
            "Epoch 30/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 1.0155 - accuracy: 0.5663 - val_loss: 1.0238 - val_accuracy: 0.5546\n",
            "Epoch 31/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 1.0104 - accuracy: 0.5707 - val_loss: 1.0046 - val_accuracy: 0.5623\n",
            "Epoch 32/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 1.0143 - accuracy: 0.5665 - val_loss: 0.9853 - val_accuracy: 0.5829\n",
            "Epoch 33/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 1.0007 - accuracy: 0.5700 - val_loss: 1.0165 - val_accuracy: 0.5659\n",
            "Epoch 34/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 1.0139 - accuracy: 0.5689 - val_loss: 0.9971 - val_accuracy: 0.5736\n",
            "Epoch 35/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0021 - accuracy: 0.5799 - val_loss: 1.0180 - val_accuracy: 0.5634\n",
            "Epoch 36/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.9905 - accuracy: 0.5793 - val_loss: 0.9861 - val_accuracy: 0.5890\n",
            "Epoch 37/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 1.0027 - accuracy: 0.5751 - val_loss: 0.9628 - val_accuracy: 0.5705\n",
            "Epoch 38/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 1.0135 - accuracy: 0.5665 - val_loss: 1.0462 - val_accuracy: 0.5469\n",
            "Epoch 39/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0206 - accuracy: 0.5588 - val_loss: 0.9814 - val_accuracy: 0.5685\n",
            "Epoch 40/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.9973 - accuracy: 0.5771 - val_loss: 1.0058 - val_accuracy: 0.5634\n",
            "Epoch 41/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 0.9897 - accuracy: 0.5791 - val_loss: 1.0063 - val_accuracy: 0.5690\n",
            "Epoch 42/2000\n",
            "4547/4547 [==============================] - 2s 347us/step - loss: 0.9960 - accuracy: 0.5799 - val_loss: 1.0124 - val_accuracy: 0.5587\n",
            "Epoch 43/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0046 - accuracy: 0.5773 - val_loss: 0.9500 - val_accuracy: 0.5900\n",
            "Epoch 44/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.9867 - accuracy: 0.5795 - val_loss: 1.0151 - val_accuracy: 0.5587\n",
            "Epoch 45/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 0.9866 - accuracy: 0.5755 - val_loss: 0.9512 - val_accuracy: 0.5926\n",
            "Epoch 46/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.9829 - accuracy: 0.5742 - val_loss: 0.9654 - val_accuracy: 0.5803\n",
            "Epoch 47/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.9875 - accuracy: 0.5824 - val_loss: 0.9650 - val_accuracy: 0.5880\n",
            "Epoch 48/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 0.9794 - accuracy: 0.5850 - val_loss: 1.0017 - val_accuracy: 0.5618\n",
            "Epoch 49/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.9901 - accuracy: 0.5813 - val_loss: 0.9571 - val_accuracy: 0.5865\n",
            "Epoch 50/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9824 - accuracy: 0.5799 - val_loss: 0.9985 - val_accuracy: 0.5711\n",
            "Epoch 51/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.9764 - accuracy: 0.5835 - val_loss: 0.9685 - val_accuracy: 0.5854\n",
            "Epoch 52/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9824 - accuracy: 0.5777 - val_loss: 0.9510 - val_accuracy: 0.5957\n",
            "Epoch 53/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9676 - accuracy: 0.5927 - val_loss: 0.9281 - val_accuracy: 0.6101\n",
            "Epoch 54/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9661 - accuracy: 0.5914 - val_loss: 0.9308 - val_accuracy: 0.5952\n",
            "Epoch 55/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9709 - accuracy: 0.5934 - val_loss: 0.9247 - val_accuracy: 0.5977\n",
            "Epoch 56/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9678 - accuracy: 0.5883 - val_loss: 1.0032 - val_accuracy: 0.5639\n",
            "Epoch 57/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 1.0047 - accuracy: 0.5707 - val_loss: 0.9416 - val_accuracy: 0.5962\n",
            "Epoch 58/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.9675 - accuracy: 0.5872 - val_loss: 0.9289 - val_accuracy: 0.5972\n",
            "Epoch 59/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.9545 - accuracy: 0.5914 - val_loss: 0.8906 - val_accuracy: 0.6095\n",
            "Epoch 60/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9476 - accuracy: 0.5949 - val_loss: 0.9073 - val_accuracy: 0.6126\n",
            "Epoch 61/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.9393 - accuracy: 0.5936 - val_loss: 0.8984 - val_accuracy: 0.6060\n",
            "Epoch 62/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.9384 - accuracy: 0.6004 - val_loss: 0.8968 - val_accuracy: 0.6126\n",
            "Epoch 63/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.9336 - accuracy: 0.5949 - val_loss: 0.9011 - val_accuracy: 0.6008\n",
            "Epoch 64/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.9273 - accuracy: 0.6026 - val_loss: 0.8922 - val_accuracy: 0.6003\n",
            "Epoch 65/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9217 - accuracy: 0.6090 - val_loss: 0.9921 - val_accuracy: 0.5844\n",
            "Epoch 66/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.9335 - accuracy: 0.5914 - val_loss: 0.9149 - val_accuracy: 0.5957\n",
            "Epoch 67/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.9421 - accuracy: 0.5936 - val_loss: 0.8901 - val_accuracy: 0.6290\n",
            "Epoch 68/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9341 - accuracy: 0.5940 - val_loss: 0.9613 - val_accuracy: 0.5834\n",
            "Epoch 69/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.9502 - accuracy: 0.5964 - val_loss: 0.8906 - val_accuracy: 0.6126\n",
            "Epoch 70/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.9283 - accuracy: 0.5967 - val_loss: 0.9744 - val_accuracy: 0.5870\n",
            "Epoch 71/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.9315 - accuracy: 0.5909 - val_loss: 0.8745 - val_accuracy: 0.6039\n",
            "Epoch 72/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.9237 - accuracy: 0.5980 - val_loss: 0.8566 - val_accuracy: 0.6193\n",
            "Epoch 73/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.8938 - accuracy: 0.6081 - val_loss: 0.9071 - val_accuracy: 0.5854\n",
            "Epoch 74/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9002 - accuracy: 0.6151 - val_loss: 0.8240 - val_accuracy: 0.6388\n",
            "Epoch 75/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.8977 - accuracy: 0.6002 - val_loss: 0.8577 - val_accuracy: 0.6080\n",
            "Epoch 76/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.8791 - accuracy: 0.6149 - val_loss: 0.8105 - val_accuracy: 0.6239\n",
            "Epoch 77/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.8661 - accuracy: 0.6219 - val_loss: 0.8163 - val_accuracy: 0.6408\n",
            "Epoch 78/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.8706 - accuracy: 0.6244 - val_loss: 0.9375 - val_accuracy: 0.5947\n",
            "Epoch 79/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.9477 - accuracy: 0.5876 - val_loss: 0.8503 - val_accuracy: 0.6337\n",
            "Epoch 80/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.8934 - accuracy: 0.6105 - val_loss: 0.8161 - val_accuracy: 0.6301\n",
            "Epoch 81/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.9005 - accuracy: 0.6105 - val_loss: 0.8286 - val_accuracy: 0.6408\n",
            "Epoch 82/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.9162 - accuracy: 0.6059 - val_loss: 0.8341 - val_accuracy: 0.6290\n",
            "Epoch 83/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.8843 - accuracy: 0.6123 - val_loss: 0.8225 - val_accuracy: 0.6301\n",
            "Epoch 84/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.8711 - accuracy: 0.6132 - val_loss: 0.7754 - val_accuracy: 0.6650\n",
            "Epoch 85/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.8640 - accuracy: 0.6204 - val_loss: 0.7691 - val_accuracy: 0.6609\n",
            "Epoch 86/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.8894 - accuracy: 0.6123 - val_loss: 1.0119 - val_accuracy: 0.5598\n",
            "Epoch 87/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.8974 - accuracy: 0.6092 - val_loss: 0.8134 - val_accuracy: 0.6506\n",
            "Epoch 88/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.8590 - accuracy: 0.6250 - val_loss: 0.7733 - val_accuracy: 0.6655\n",
            "Epoch 89/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.8506 - accuracy: 0.6349 - val_loss: 0.7714 - val_accuracy: 0.6670\n",
            "Epoch 90/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.8548 - accuracy: 0.6263 - val_loss: 0.7523 - val_accuracy: 0.6619\n",
            "Epoch 91/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.8333 - accuracy: 0.6437 - val_loss: 0.7318 - val_accuracy: 0.6819\n",
            "Epoch 92/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.8369 - accuracy: 0.6415 - val_loss: 0.7360 - val_accuracy: 0.6860\n",
            "Epoch 93/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.8061 - accuracy: 0.6450 - val_loss: 0.7261 - val_accuracy: 0.6762\n",
            "Epoch 94/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.8161 - accuracy: 0.6444 - val_loss: 0.8566 - val_accuracy: 0.6234\n",
            "Epoch 95/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.7937 - accuracy: 0.6499 - val_loss: 0.7210 - val_accuracy: 0.6896\n",
            "Epoch 96/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7985 - accuracy: 0.6536 - val_loss: 0.6889 - val_accuracy: 0.7127\n",
            "Epoch 97/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7781 - accuracy: 0.6666 - val_loss: 0.6526 - val_accuracy: 0.7234\n",
            "Epoch 98/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.7931 - accuracy: 0.6576 - val_loss: 0.7805 - val_accuracy: 0.6516\n",
            "Epoch 99/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.8083 - accuracy: 0.6455 - val_loss: 0.6533 - val_accuracy: 0.7250\n",
            "Epoch 100/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.8608 - accuracy: 0.6149 - val_loss: 0.7460 - val_accuracy: 0.6896\n",
            "Epoch 101/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7981 - accuracy: 0.6530 - val_loss: 0.6817 - val_accuracy: 0.7081\n",
            "Epoch 102/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7761 - accuracy: 0.6640 - val_loss: 0.6572 - val_accuracy: 0.7224\n",
            "Epoch 103/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.8296 - accuracy: 0.6371 - val_loss: 0.7398 - val_accuracy: 0.6860\n",
            "Epoch 104/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.8073 - accuracy: 0.6420 - val_loss: 0.6793 - val_accuracy: 0.7116\n",
            "Epoch 105/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.7885 - accuracy: 0.6516 - val_loss: 0.6606 - val_accuracy: 0.7158\n",
            "Epoch 106/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.7826 - accuracy: 0.6620 - val_loss: 0.7068 - val_accuracy: 0.7050\n",
            "Epoch 107/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7662 - accuracy: 0.6677 - val_loss: 0.6631 - val_accuracy: 0.7373\n",
            "Epoch 108/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.7492 - accuracy: 0.6714 - val_loss: 0.6196 - val_accuracy: 0.7347\n",
            "Epoch 109/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.7297 - accuracy: 0.6870 - val_loss: 0.5838 - val_accuracy: 0.7455\n",
            "Epoch 110/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7098 - accuracy: 0.6978 - val_loss: 0.6605 - val_accuracy: 0.7332\n",
            "Epoch 111/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.7354 - accuracy: 0.6851 - val_loss: 0.5902 - val_accuracy: 0.7732\n",
            "Epoch 112/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.7137 - accuracy: 0.6877 - val_loss: 0.5690 - val_accuracy: 0.7696\n",
            "Epoch 113/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6958 - accuracy: 0.6998 - val_loss: 0.6246 - val_accuracy: 0.7388\n",
            "Epoch 114/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.6823 - accuracy: 0.7053 - val_loss: 0.5460 - val_accuracy: 0.7773\n",
            "Epoch 115/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.6779 - accuracy: 0.7141 - val_loss: 0.5297 - val_accuracy: 0.7948\n",
            "Epoch 116/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.6626 - accuracy: 0.7172 - val_loss: 0.5512 - val_accuracy: 0.7907\n",
            "Epoch 117/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6711 - accuracy: 0.7117 - val_loss: 0.5287 - val_accuracy: 0.7840\n",
            "Epoch 118/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6772 - accuracy: 0.7130 - val_loss: 0.5060 - val_accuracy: 0.8019\n",
            "Epoch 119/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.6497 - accuracy: 0.7273 - val_loss: 0.4908 - val_accuracy: 0.8153\n",
            "Epoch 120/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6175 - accuracy: 0.7420 - val_loss: 0.5044 - val_accuracy: 0.8050\n",
            "Epoch 121/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.6100 - accuracy: 0.7480 - val_loss: 0.4675 - val_accuracy: 0.8256\n",
            "Epoch 122/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.6411 - accuracy: 0.7258 - val_loss: 0.5129 - val_accuracy: 0.7907\n",
            "Epoch 123/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.6199 - accuracy: 0.7374 - val_loss: 0.4841 - val_accuracy: 0.8004\n",
            "Epoch 124/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6275 - accuracy: 0.7387 - val_loss: 0.4394 - val_accuracy: 0.8425\n",
            "Epoch 125/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.6443 - accuracy: 0.7302 - val_loss: 0.5107 - val_accuracy: 0.7973\n",
            "Epoch 126/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.6241 - accuracy: 0.7418 - val_loss: 0.5399 - val_accuracy: 0.7989\n",
            "Epoch 127/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.6367 - accuracy: 0.7328 - val_loss: 0.4732 - val_accuracy: 0.8297\n",
            "Epoch 128/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.6518 - accuracy: 0.7242 - val_loss: 0.4790 - val_accuracy: 0.8030\n",
            "Epoch 129/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.6110 - accuracy: 0.7429 - val_loss: 0.4505 - val_accuracy: 0.8297\n",
            "Epoch 130/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.6437 - accuracy: 0.7352 - val_loss: 0.5890 - val_accuracy: 0.7609\n",
            "Epoch 131/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.6188 - accuracy: 0.7352 - val_loss: 0.4523 - val_accuracy: 0.8271\n",
            "Epoch 132/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.5613 - accuracy: 0.7662 - val_loss: 0.3934 - val_accuracy: 0.8645\n",
            "Epoch 133/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.5801 - accuracy: 0.7550 - val_loss: 0.4274 - val_accuracy: 0.8291\n",
            "Epoch 134/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.5608 - accuracy: 0.7651 - val_loss: 0.4029 - val_accuracy: 0.8435\n",
            "Epoch 135/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.7690 - accuracy: 0.6741 - val_loss: 0.5775 - val_accuracy: 0.7671\n",
            "Epoch 136/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.6617 - accuracy: 0.7260 - val_loss: 0.5028 - val_accuracy: 0.7968\n",
            "Epoch 137/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.6139 - accuracy: 0.7392 - val_loss: 0.4289 - val_accuracy: 0.8404\n",
            "Epoch 138/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.6088 - accuracy: 0.7383 - val_loss: 0.5403 - val_accuracy: 0.7732\n",
            "Epoch 139/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.6205 - accuracy: 0.7321 - val_loss: 0.4322 - val_accuracy: 0.8420\n",
            "Epoch 140/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.6008 - accuracy: 0.7433 - val_loss: 0.3924 - val_accuracy: 0.8594\n",
            "Epoch 141/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.5436 - accuracy: 0.7803 - val_loss: 0.3607 - val_accuracy: 0.8702\n",
            "Epoch 142/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.5587 - accuracy: 0.7739 - val_loss: 0.3440 - val_accuracy: 0.8830\n",
            "Epoch 143/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.5510 - accuracy: 0.7733 - val_loss: 0.3620 - val_accuracy: 0.8733\n",
            "Epoch 144/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.5268 - accuracy: 0.7843 - val_loss: 0.3354 - val_accuracy: 0.8881\n",
            "Epoch 145/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.5064 - accuracy: 0.7928 - val_loss: 0.3873 - val_accuracy: 0.8620\n",
            "Epoch 146/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.5502 - accuracy: 0.7752 - val_loss: 0.4071 - val_accuracy: 0.8404\n",
            "Epoch 147/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.5067 - accuracy: 0.7941 - val_loss: 0.3776 - val_accuracy: 0.8604\n",
            "Epoch 148/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.4998 - accuracy: 0.8016 - val_loss: 0.3033 - val_accuracy: 0.8917\n",
            "Epoch 149/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.4969 - accuracy: 0.7933 - val_loss: 0.2903 - val_accuracy: 0.8943\n",
            "Epoch 150/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.4555 - accuracy: 0.8137 - val_loss: 0.3175 - val_accuracy: 0.8856\n",
            "Epoch 151/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.4765 - accuracy: 0.8104 - val_loss: 0.2647 - val_accuracy: 0.9071\n",
            "Epoch 152/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.4967 - accuracy: 0.7948 - val_loss: 0.4708 - val_accuracy: 0.8220\n",
            "Epoch 153/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.5226 - accuracy: 0.7849 - val_loss: 0.2930 - val_accuracy: 0.8974\n",
            "Epoch 154/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.4621 - accuracy: 0.8128 - val_loss: 0.2599 - val_accuracy: 0.9133\n",
            "Epoch 155/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.4708 - accuracy: 0.8115 - val_loss: 0.2508 - val_accuracy: 0.9179\n",
            "Epoch 156/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.4619 - accuracy: 0.8095 - val_loss: 0.2646 - val_accuracy: 0.9164\n",
            "Epoch 157/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.4949 - accuracy: 0.7979 - val_loss: 0.2608 - val_accuracy: 0.9112\n",
            "Epoch 158/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.4441 - accuracy: 0.8243 - val_loss: 0.2270 - val_accuracy: 0.9164\n",
            "Epoch 159/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.4872 - accuracy: 0.8084 - val_loss: 0.2514 - val_accuracy: 0.9338\n",
            "Epoch 160/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.4221 - accuracy: 0.8269 - val_loss: 0.2457 - val_accuracy: 0.9205\n",
            "Epoch 161/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.4256 - accuracy: 0.8342 - val_loss: 0.2257 - val_accuracy: 0.9174\n",
            "Epoch 162/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.4472 - accuracy: 0.8291 - val_loss: 0.2937 - val_accuracy: 0.8866\n",
            "Epoch 163/2000\n",
            "4547/4547 [==============================] - 2s 352us/step - loss: 0.4178 - accuracy: 0.8359 - val_loss: 0.2217 - val_accuracy: 0.9236\n",
            "Epoch 164/2000\n",
            "4547/4547 [==============================] - 2s 356us/step - loss: 0.6229 - accuracy: 0.7493 - val_loss: 0.3631 - val_accuracy: 0.8697\n",
            "Epoch 165/2000\n",
            "4547/4547 [==============================] - 2s 355us/step - loss: 0.4774 - accuracy: 0.8128 - val_loss: 0.2604 - val_accuracy: 0.9143\n",
            "Epoch 166/2000\n",
            "4547/4547 [==============================] - 2s 352us/step - loss: 0.4785 - accuracy: 0.8054 - val_loss: 0.2558 - val_accuracy: 0.9189\n",
            "Epoch 167/2000\n",
            "4547/4547 [==============================] - 2s 355us/step - loss: 0.4712 - accuracy: 0.8062 - val_loss: 0.2523 - val_accuracy: 0.9092\n",
            "Epoch 168/2000\n",
            "4547/4547 [==============================] - 2s 349us/step - loss: 0.4172 - accuracy: 0.8329 - val_loss: 0.2181 - val_accuracy: 0.9343\n",
            "Epoch 169/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.4201 - accuracy: 0.8351 - val_loss: 0.2319 - val_accuracy: 0.9189\n",
            "Epoch 170/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.4217 - accuracy: 0.8337 - val_loss: 0.1971 - val_accuracy: 0.9456\n",
            "Epoch 171/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.4167 - accuracy: 0.8359 - val_loss: 0.2015 - val_accuracy: 0.9410\n",
            "Epoch 172/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.3952 - accuracy: 0.8432 - val_loss: 0.1914 - val_accuracy: 0.9436\n",
            "Epoch 173/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.3868 - accuracy: 0.8533 - val_loss: 0.2043 - val_accuracy: 0.9410\n",
            "Epoch 174/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.3791 - accuracy: 0.8511 - val_loss: 0.1945 - val_accuracy: 0.9441\n",
            "Epoch 175/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3681 - accuracy: 0.8608 - val_loss: 0.2957 - val_accuracy: 0.9395\n",
            "Epoch 176/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.3908 - accuracy: 0.8469 - val_loss: 0.1966 - val_accuracy: 0.9333\n",
            "Epoch 177/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3548 - accuracy: 0.8590 - val_loss: 0.1593 - val_accuracy: 0.9543\n",
            "Epoch 178/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3391 - accuracy: 0.8663 - val_loss: 0.1903 - val_accuracy: 0.9354\n",
            "Epoch 179/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.3980 - accuracy: 0.8441 - val_loss: 0.1824 - val_accuracy: 0.9477\n",
            "Epoch 180/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.4211 - accuracy: 0.8364 - val_loss: 0.1812 - val_accuracy: 0.9405\n",
            "Epoch 181/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.3747 - accuracy: 0.8537 - val_loss: 0.1643 - val_accuracy: 0.9543\n",
            "Epoch 182/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3726 - accuracy: 0.8557 - val_loss: 0.1929 - val_accuracy: 0.9415\n",
            "Epoch 183/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3515 - accuracy: 0.8612 - val_loss: 0.1802 - val_accuracy: 0.9559\n",
            "Epoch 184/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.3495 - accuracy: 0.8650 - val_loss: 0.2005 - val_accuracy: 0.9477\n",
            "Epoch 185/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.3689 - accuracy: 0.8513 - val_loss: 0.1440 - val_accuracy: 0.9595\n",
            "Epoch 186/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.3112 - accuracy: 0.8751 - val_loss: 0.1348 - val_accuracy: 0.9605\n",
            "Epoch 187/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.3087 - accuracy: 0.8812 - val_loss: 0.1320 - val_accuracy: 0.9569\n",
            "Epoch 188/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3208 - accuracy: 0.8782 - val_loss: 0.1407 - val_accuracy: 0.9497\n",
            "Epoch 189/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.3654 - accuracy: 0.8656 - val_loss: 0.1461 - val_accuracy: 0.9543\n",
            "Epoch 190/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3253 - accuracy: 0.8779 - val_loss: 0.1766 - val_accuracy: 0.9364\n",
            "Epoch 191/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3171 - accuracy: 0.8775 - val_loss: 0.1466 - val_accuracy: 0.9584\n",
            "Epoch 192/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3126 - accuracy: 0.8793 - val_loss: 0.1596 - val_accuracy: 0.9456\n",
            "Epoch 193/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3323 - accuracy: 0.8685 - val_loss: 0.1503 - val_accuracy: 0.9528\n",
            "Epoch 194/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.3232 - accuracy: 0.8773 - val_loss: 0.1606 - val_accuracy: 0.9492\n",
            "Epoch 195/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.5846 - accuracy: 0.7755 - val_loss: 0.3280 - val_accuracy: 0.8866\n",
            "Epoch 196/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.4274 - accuracy: 0.8318 - val_loss: 0.2028 - val_accuracy: 0.9359\n",
            "Epoch 197/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.4296 - accuracy: 0.8315 - val_loss: 0.1864 - val_accuracy: 0.9456\n",
            "Epoch 198/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.3896 - accuracy: 0.8445 - val_loss: 0.1608 - val_accuracy: 0.9543\n",
            "Epoch 199/2000\n",
            "4547/4547 [==============================] - 2s 346us/step - loss: 0.3593 - accuracy: 0.8632 - val_loss: 0.1801 - val_accuracy: 0.9436\n",
            "Epoch 200/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.3517 - accuracy: 0.8645 - val_loss: 0.1382 - val_accuracy: 0.9579\n",
            "Epoch 201/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.3424 - accuracy: 0.8702 - val_loss: 0.1294 - val_accuracy: 0.9636\n",
            "Epoch 202/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3552 - accuracy: 0.8650 - val_loss: 0.2006 - val_accuracy: 0.9277\n",
            "Epoch 203/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.3458 - accuracy: 0.8636 - val_loss: 0.1525 - val_accuracy: 0.9528\n",
            "Epoch 204/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.3080 - accuracy: 0.8804 - val_loss: 0.1232 - val_accuracy: 0.9625\n",
            "Epoch 205/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2862 - accuracy: 0.8958 - val_loss: 0.1026 - val_accuracy: 0.9692\n",
            "Epoch 206/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.3036 - accuracy: 0.8839 - val_loss: 0.1103 - val_accuracy: 0.9677\n",
            "Epoch 207/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2869 - accuracy: 0.8861 - val_loss: 0.1060 - val_accuracy: 0.9723\n",
            "Epoch 208/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.3230 - accuracy: 0.8768 - val_loss: 0.1079 - val_accuracy: 0.9697\n",
            "Epoch 209/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2903 - accuracy: 0.8878 - val_loss: 0.1022 - val_accuracy: 0.9769\n",
            "Epoch 210/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2751 - accuracy: 0.8914 - val_loss: 0.1071 - val_accuracy: 0.9723\n",
            "Epoch 211/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2789 - accuracy: 0.8889 - val_loss: 0.0918 - val_accuracy: 0.9718\n",
            "Epoch 212/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.3081 - accuracy: 0.8832 - val_loss: 0.1137 - val_accuracy: 0.9682\n",
            "Epoch 213/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3213 - accuracy: 0.8683 - val_loss: 0.1099 - val_accuracy: 0.9708\n",
            "Epoch 214/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2817 - accuracy: 0.8940 - val_loss: 0.0900 - val_accuracy: 0.9754\n",
            "Epoch 215/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2561 - accuracy: 0.9017 - val_loss: 0.0910 - val_accuracy: 0.9759\n",
            "Epoch 216/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.3278 - accuracy: 0.8738 - val_loss: 0.3795 - val_accuracy: 0.8620\n",
            "Epoch 217/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.3200 - accuracy: 0.8786 - val_loss: 0.0982 - val_accuracy: 0.9743\n",
            "Epoch 218/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.3466 - accuracy: 0.8632 - val_loss: 0.1194 - val_accuracy: 0.9656\n",
            "Epoch 219/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2820 - accuracy: 0.8938 - val_loss: 0.1227 - val_accuracy: 0.9631\n",
            "Epoch 220/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2774 - accuracy: 0.8916 - val_loss: 0.1128 - val_accuracy: 0.9672\n",
            "Epoch 221/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.2651 - accuracy: 0.9004 - val_loss: 0.0938 - val_accuracy: 0.9738\n",
            "Epoch 222/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2675 - accuracy: 0.8995 - val_loss: 0.0970 - val_accuracy: 0.9708\n",
            "Epoch 223/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2580 - accuracy: 0.8988 - val_loss: 0.0869 - val_accuracy: 0.9774\n",
            "Epoch 224/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.2578 - accuracy: 0.8971 - val_loss: 0.0818 - val_accuracy: 0.9800\n",
            "Epoch 225/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2489 - accuracy: 0.9059 - val_loss: 0.0761 - val_accuracy: 0.9831\n",
            "Epoch 226/2000\n",
            "4547/4547 [==============================] - 2s 330us/step - loss: 0.2530 - accuracy: 0.9041 - val_loss: 0.1096 - val_accuracy: 0.9697\n",
            "Epoch 227/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2931 - accuracy: 0.8859 - val_loss: 0.0917 - val_accuracy: 0.9774\n",
            "Epoch 228/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2748 - accuracy: 0.8962 - val_loss: 0.0904 - val_accuracy: 0.9708\n",
            "Epoch 229/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2850 - accuracy: 0.8916 - val_loss: 0.0887 - val_accuracy: 0.9826\n",
            "Epoch 230/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2717 - accuracy: 0.8958 - val_loss: 0.1011 - val_accuracy: 0.9733\n",
            "Epoch 231/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2662 - accuracy: 0.8993 - val_loss: 0.0735 - val_accuracy: 0.9826\n",
            "Epoch 232/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2973 - accuracy: 0.8830 - val_loss: 0.1068 - val_accuracy: 0.9723\n",
            "Epoch 233/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2835 - accuracy: 0.8969 - val_loss: 0.1045 - val_accuracy: 0.9692\n",
            "Epoch 234/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.2824 - accuracy: 0.8896 - val_loss: 0.0989 - val_accuracy: 0.9754\n",
            "Epoch 235/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2576 - accuracy: 0.9046 - val_loss: 0.0767 - val_accuracy: 0.9826\n",
            "Epoch 236/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2287 - accuracy: 0.9169 - val_loss: 0.0580 - val_accuracy: 0.9877\n",
            "Epoch 237/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2525 - accuracy: 0.9041 - val_loss: 0.0861 - val_accuracy: 0.9795\n",
            "Epoch 238/2000\n",
            "4547/4547 [==============================] - 2s 330us/step - loss: 0.2630 - accuracy: 0.9013 - val_loss: 0.0978 - val_accuracy: 0.9749\n",
            "Epoch 239/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2638 - accuracy: 0.8980 - val_loss: 0.0821 - val_accuracy: 0.9815\n",
            "Epoch 240/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2564 - accuracy: 0.9004 - val_loss: 0.1097 - val_accuracy: 0.9682\n",
            "Epoch 241/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2319 - accuracy: 0.9127 - val_loss: 0.0715 - val_accuracy: 0.9841\n",
            "Epoch 242/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2414 - accuracy: 0.9059 - val_loss: 0.0776 - val_accuracy: 0.9800\n",
            "Epoch 243/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2287 - accuracy: 0.9153 - val_loss: 0.0686 - val_accuracy: 0.9846\n",
            "Epoch 244/2000\n",
            "4547/4547 [==============================] - 1s 330us/step - loss: 0.2411 - accuracy: 0.9092 - val_loss: 0.0840 - val_accuracy: 0.9769\n",
            "Epoch 245/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2302 - accuracy: 0.9112 - val_loss: 0.0752 - val_accuracy: 0.9774\n",
            "Epoch 246/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.5134 - accuracy: 0.8018 - val_loss: 0.2651 - val_accuracy: 0.9082\n",
            "Epoch 247/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.3373 - accuracy: 0.8702 - val_loss: 0.1118 - val_accuracy: 0.9713\n",
            "Epoch 248/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2901 - accuracy: 0.8918 - val_loss: 0.0935 - val_accuracy: 0.9764\n",
            "Epoch 249/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.2755 - accuracy: 0.8944 - val_loss: 0.0941 - val_accuracy: 0.9743\n",
            "Epoch 250/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2649 - accuracy: 0.9006 - val_loss: 0.0836 - val_accuracy: 0.9779\n",
            "Epoch 251/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 0.2694 - accuracy: 0.9002 - val_loss: 0.0866 - val_accuracy: 0.9795\n",
            "Epoch 252/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2779 - accuracy: 0.8898 - val_loss: 0.1305 - val_accuracy: 0.9641\n",
            "Epoch 253/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2691 - accuracy: 0.8986 - val_loss: 0.0698 - val_accuracy: 0.9851\n",
            "Epoch 254/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2426 - accuracy: 0.9094 - val_loss: 0.0697 - val_accuracy: 0.9826\n",
            "Epoch 255/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2281 - accuracy: 0.9158 - val_loss: 0.0649 - val_accuracy: 0.9836\n",
            "Epoch 256/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2519 - accuracy: 0.9021 - val_loss: 0.0748 - val_accuracy: 0.9836\n",
            "Epoch 257/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2165 - accuracy: 0.9155 - val_loss: 0.0625 - val_accuracy: 0.9882\n",
            "Epoch 258/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2773 - accuracy: 0.9008 - val_loss: 0.0976 - val_accuracy: 0.9749\n",
            "Epoch 259/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.2780 - accuracy: 0.8885 - val_loss: 0.0780 - val_accuracy: 0.9805\n",
            "Epoch 260/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2393 - accuracy: 0.9118 - val_loss: 0.0638 - val_accuracy: 0.9841\n",
            "Epoch 261/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2283 - accuracy: 0.9138 - val_loss: 0.0716 - val_accuracy: 0.9846\n",
            "Epoch 262/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2180 - accuracy: 0.9184 - val_loss: 0.0638 - val_accuracy: 0.9892\n",
            "Epoch 263/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2188 - accuracy: 0.9204 - val_loss: 0.0717 - val_accuracy: 0.9815\n",
            "Epoch 264/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2651 - accuracy: 0.8975 - val_loss: 0.1223 - val_accuracy: 0.9702\n",
            "Epoch 265/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2487 - accuracy: 0.9061 - val_loss: 0.0743 - val_accuracy: 0.9815\n",
            "Epoch 266/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2454 - accuracy: 0.9083 - val_loss: 0.0852 - val_accuracy: 0.9779\n",
            "Epoch 267/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2338 - accuracy: 0.9083 - val_loss: 0.0634 - val_accuracy: 0.9867\n",
            "Epoch 268/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2223 - accuracy: 0.9180 - val_loss: 0.0765 - val_accuracy: 0.9851\n",
            "Epoch 269/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3075 - accuracy: 0.8821 - val_loss: 0.0976 - val_accuracy: 0.9774\n",
            "Epoch 270/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2351 - accuracy: 0.9133 - val_loss: 0.0573 - val_accuracy: 0.9903\n",
            "Epoch 271/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2274 - accuracy: 0.9096 - val_loss: 0.0543 - val_accuracy: 0.9897\n",
            "Epoch 272/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2132 - accuracy: 0.9252 - val_loss: 0.0556 - val_accuracy: 0.9887\n",
            "Epoch 273/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2168 - accuracy: 0.9195 - val_loss: 0.0781 - val_accuracy: 0.9820\n",
            "Epoch 274/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1916 - accuracy: 0.9285 - val_loss: 0.0540 - val_accuracy: 0.9923\n",
            "Epoch 275/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2703 - accuracy: 0.9028 - val_loss: 0.0638 - val_accuracy: 0.9861\n",
            "Epoch 276/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2372 - accuracy: 0.9090 - val_loss: 0.0691 - val_accuracy: 0.9877\n",
            "Epoch 277/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2135 - accuracy: 0.9208 - val_loss: 0.0623 - val_accuracy: 0.9913\n",
            "Epoch 278/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2095 - accuracy: 0.9219 - val_loss: 0.0558 - val_accuracy: 0.9877\n",
            "Epoch 279/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2049 - accuracy: 0.9228 - val_loss: 0.0620 - val_accuracy: 0.9856\n",
            "Epoch 280/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2405 - accuracy: 0.9098 - val_loss: 0.0628 - val_accuracy: 0.9872\n",
            "Epoch 281/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2048 - accuracy: 0.9237 - val_loss: 0.0596 - val_accuracy: 0.9872\n",
            "Epoch 282/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2373 - accuracy: 0.9149 - val_loss: 0.0603 - val_accuracy: 0.9887\n",
            "Epoch 283/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2304 - accuracy: 0.9155 - val_loss: 0.0517 - val_accuracy: 0.9897\n",
            "Epoch 284/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2105 - accuracy: 0.9221 - val_loss: 0.0813 - val_accuracy: 0.9764\n",
            "Epoch 285/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2213 - accuracy: 0.9175 - val_loss: 0.0505 - val_accuracy: 0.9887\n",
            "Epoch 286/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2180 - accuracy: 0.9155 - val_loss: 0.0599 - val_accuracy: 0.9892\n",
            "Epoch 287/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1925 - accuracy: 0.9265 - val_loss: 0.0511 - val_accuracy: 0.9903\n",
            "Epoch 288/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1895 - accuracy: 0.9331 - val_loss: 0.0591 - val_accuracy: 0.9861\n",
            "Epoch 289/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.2079 - accuracy: 0.9270 - val_loss: 0.0702 - val_accuracy: 0.9810\n",
            "Epoch 290/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2108 - accuracy: 0.9263 - val_loss: 0.0561 - val_accuracy: 0.9856\n",
            "Epoch 291/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1920 - accuracy: 0.9362 - val_loss: 0.0480 - val_accuracy: 0.9897\n",
            "Epoch 292/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2054 - accuracy: 0.9279 - val_loss: 0.0536 - val_accuracy: 0.9877\n",
            "Epoch 293/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1989 - accuracy: 0.9314 - val_loss: 0.0611 - val_accuracy: 0.9841\n",
            "Epoch 294/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2039 - accuracy: 0.9235 - val_loss: 0.0476 - val_accuracy: 0.9897\n",
            "Epoch 295/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1893 - accuracy: 0.9283 - val_loss: 0.0401 - val_accuracy: 0.9928\n",
            "Epoch 296/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1808 - accuracy: 0.9331 - val_loss: 0.0526 - val_accuracy: 0.9856\n",
            "Epoch 297/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2046 - accuracy: 0.9235 - val_loss: 0.0482 - val_accuracy: 0.9887\n",
            "Epoch 298/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1674 - accuracy: 0.9437 - val_loss: 0.0556 - val_accuracy: 0.9851\n",
            "Epoch 299/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2126 - accuracy: 0.9219 - val_loss: 0.0642 - val_accuracy: 0.9861\n",
            "Epoch 300/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2863 - accuracy: 0.9010 - val_loss: 0.0633 - val_accuracy: 0.9851\n",
            "Epoch 301/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 0.2200 - accuracy: 0.9162 - val_loss: 0.0749 - val_accuracy: 0.9820\n",
            "Epoch 302/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2022 - accuracy: 0.9213 - val_loss: 0.1018 - val_accuracy: 0.9682\n",
            "Epoch 303/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.2028 - accuracy: 0.9257 - val_loss: 0.0513 - val_accuracy: 0.9897\n",
            "Epoch 304/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1910 - accuracy: 0.9290 - val_loss: 0.0561 - val_accuracy: 0.9861\n",
            "Epoch 305/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1768 - accuracy: 0.9347 - val_loss: 0.0468 - val_accuracy: 0.9882\n",
            "Epoch 306/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3224 - accuracy: 0.8867 - val_loss: 0.0598 - val_accuracy: 0.9861\n",
            "Epoch 307/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.2108 - accuracy: 0.9204 - val_loss: 0.0624 - val_accuracy: 0.9861\n",
            "Epoch 308/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2807 - accuracy: 0.8999 - val_loss: 0.0771 - val_accuracy: 0.9831\n",
            "Epoch 309/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2718 - accuracy: 0.8991 - val_loss: 0.0656 - val_accuracy: 0.9851\n",
            "Epoch 310/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1896 - accuracy: 0.9312 - val_loss: 0.0642 - val_accuracy: 0.9841\n",
            "Epoch 311/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2042 - accuracy: 0.9228 - val_loss: 0.0474 - val_accuracy: 0.9851\n",
            "Epoch 312/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1748 - accuracy: 0.9380 - val_loss: 0.0587 - val_accuracy: 0.9867\n",
            "Epoch 313/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1956 - accuracy: 0.9316 - val_loss: 0.0540 - val_accuracy: 0.9892\n",
            "Epoch 314/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2340 - accuracy: 0.9140 - val_loss: 0.0583 - val_accuracy: 0.9882\n",
            "Epoch 315/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2041 - accuracy: 0.9230 - val_loss: 0.0812 - val_accuracy: 0.9785\n",
            "Epoch 316/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1949 - accuracy: 0.9298 - val_loss: 0.0401 - val_accuracy: 0.9933\n",
            "Epoch 317/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1734 - accuracy: 0.9360 - val_loss: 0.0383 - val_accuracy: 0.9887\n",
            "Epoch 318/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2160 - accuracy: 0.9188 - val_loss: 0.0520 - val_accuracy: 0.9887\n",
            "Epoch 319/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2178 - accuracy: 0.9160 - val_loss: 0.0905 - val_accuracy: 0.9743\n",
            "Epoch 320/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2036 - accuracy: 0.9261 - val_loss: 0.0447 - val_accuracy: 0.9923\n",
            "Epoch 321/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1819 - accuracy: 0.9349 - val_loss: 0.0440 - val_accuracy: 0.9877\n",
            "Epoch 322/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1565 - accuracy: 0.9426 - val_loss: 0.0418 - val_accuracy: 0.9913\n",
            "Epoch 323/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1731 - accuracy: 0.9358 - val_loss: 0.0445 - val_accuracy: 0.9872\n",
            "Epoch 324/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1984 - accuracy: 0.9259 - val_loss: 0.0498 - val_accuracy: 0.9923\n",
            "Epoch 325/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1626 - accuracy: 0.9353 - val_loss: 0.0990 - val_accuracy: 0.9661\n",
            "Epoch 326/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2209 - accuracy: 0.9221 - val_loss: 0.1692 - val_accuracy: 0.9328\n",
            "Epoch 327/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2735 - accuracy: 0.8986 - val_loss: 0.1168 - val_accuracy: 0.9610\n",
            "Epoch 328/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2043 - accuracy: 0.9221 - val_loss: 0.0457 - val_accuracy: 0.9882\n",
            "Epoch 329/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2580 - accuracy: 0.9002 - val_loss: 0.0593 - val_accuracy: 0.9872\n",
            "Epoch 330/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.2020 - accuracy: 0.9257 - val_loss: 0.0529 - val_accuracy: 0.9887\n",
            "Epoch 331/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2589 - accuracy: 0.9019 - val_loss: 0.0872 - val_accuracy: 0.9733\n",
            "Epoch 332/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1985 - accuracy: 0.9237 - val_loss: 0.0500 - val_accuracy: 0.9897\n",
            "Epoch 333/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.3700 - accuracy: 0.8672 - val_loss: 0.1168 - val_accuracy: 0.9769\n",
            "Epoch 334/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2365 - accuracy: 0.9120 - val_loss: 0.0583 - val_accuracy: 0.9872\n",
            "Epoch 335/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2194 - accuracy: 0.9180 - val_loss: 0.0508 - val_accuracy: 0.9882\n",
            "Epoch 336/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2187 - accuracy: 0.9195 - val_loss: 0.0520 - val_accuracy: 0.9872\n",
            "Epoch 337/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2047 - accuracy: 0.9274 - val_loss: 0.0444 - val_accuracy: 0.9908\n",
            "Epoch 338/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1722 - accuracy: 0.9358 - val_loss: 0.0396 - val_accuracy: 0.9923\n",
            "Epoch 339/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1991 - accuracy: 0.9290 - val_loss: 0.0452 - val_accuracy: 0.9928\n",
            "Epoch 340/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1836 - accuracy: 0.9298 - val_loss: 0.0390 - val_accuracy: 0.9928\n",
            "Epoch 341/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1885 - accuracy: 0.9303 - val_loss: 0.0434 - val_accuracy: 0.9887\n",
            "Epoch 342/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2082 - accuracy: 0.9248 - val_loss: 0.0453 - val_accuracy: 0.9908\n",
            "Epoch 343/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1703 - accuracy: 0.9400 - val_loss: 0.0405 - val_accuracy: 0.9897\n",
            "Epoch 344/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1722 - accuracy: 0.9367 - val_loss: 0.0384 - val_accuracy: 0.9923\n",
            "Epoch 345/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1933 - accuracy: 0.9283 - val_loss: 0.0428 - val_accuracy: 0.9944\n",
            "Epoch 346/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1938 - accuracy: 0.9287 - val_loss: 0.0455 - val_accuracy: 0.9903\n",
            "Epoch 347/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1738 - accuracy: 0.9364 - val_loss: 0.0420 - val_accuracy: 0.9882\n",
            "Epoch 348/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1689 - accuracy: 0.9362 - val_loss: 0.0337 - val_accuracy: 0.9933\n",
            "Epoch 349/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1555 - accuracy: 0.9417 - val_loss: 0.0364 - val_accuracy: 0.9923\n",
            "Epoch 350/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1596 - accuracy: 0.9402 - val_loss: 0.0328 - val_accuracy: 0.9938\n",
            "Epoch 351/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.1756 - accuracy: 0.9364 - val_loss: 0.0332 - val_accuracy: 0.9933\n",
            "Epoch 352/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.1568 - accuracy: 0.9402 - val_loss: 0.0335 - val_accuracy: 0.9892\n",
            "Epoch 353/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1930 - accuracy: 0.9296 - val_loss: 0.0453 - val_accuracy: 0.9887\n",
            "Epoch 354/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.2901 - accuracy: 0.8944 - val_loss: 0.5079 - val_accuracy: 0.8189\n",
            "Epoch 355/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.2746 - accuracy: 0.8964 - val_loss: 0.0816 - val_accuracy: 0.9774\n",
            "Epoch 356/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.2274 - accuracy: 0.9160 - val_loss: 0.0559 - val_accuracy: 0.9861\n",
            "Epoch 357/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.2083 - accuracy: 0.9210 - val_loss: 0.0530 - val_accuracy: 0.9887\n",
            "Epoch 358/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1876 - accuracy: 0.9323 - val_loss: 0.0439 - val_accuracy: 0.9897\n",
            "Epoch 359/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.2647 - accuracy: 0.8988 - val_loss: 0.0635 - val_accuracy: 0.9872\n",
            "Epoch 360/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1898 - accuracy: 0.9327 - val_loss: 0.0414 - val_accuracy: 0.9897\n",
            "Epoch 361/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1948 - accuracy: 0.9301 - val_loss: 0.0387 - val_accuracy: 0.9903\n",
            "Epoch 362/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2070 - accuracy: 0.9320 - val_loss: 0.0398 - val_accuracy: 0.9897\n",
            "Epoch 363/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1854 - accuracy: 0.9314 - val_loss: 0.0385 - val_accuracy: 0.9897\n",
            "Epoch 364/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1492 - accuracy: 0.9448 - val_loss: 0.0336 - val_accuracy: 0.9933\n",
            "Epoch 365/2000\n",
            "4547/4547 [==============================] - 2s 353us/step - loss: 0.1750 - accuracy: 0.9371 - val_loss: 0.0352 - val_accuracy: 0.9949\n",
            "Epoch 366/2000\n",
            "4547/4547 [==============================] - 2s 350us/step - loss: 0.1483 - accuracy: 0.9466 - val_loss: 0.0301 - val_accuracy: 0.9944\n",
            "Epoch 367/2000\n",
            "4547/4547 [==============================] - 2s 349us/step - loss: 0.1698 - accuracy: 0.9367 - val_loss: 0.0389 - val_accuracy: 0.9913\n",
            "Epoch 368/2000\n",
            "4547/4547 [==============================] - 2s 354us/step - loss: 0.1575 - accuracy: 0.9455 - val_loss: 0.0386 - val_accuracy: 0.9918\n",
            "Epoch 369/2000\n",
            "4547/4547 [==============================] - 2s 353us/step - loss: 0.1418 - accuracy: 0.9479 - val_loss: 0.0361 - val_accuracy: 0.9928\n",
            "Epoch 370/2000\n",
            "4547/4547 [==============================] - 2s 353us/step - loss: 0.1501 - accuracy: 0.9452 - val_loss: 0.0445 - val_accuracy: 0.9897\n",
            "Epoch 371/2000\n",
            "4547/4547 [==============================] - 2s 350us/step - loss: 0.1572 - accuracy: 0.9439 - val_loss: 0.0385 - val_accuracy: 0.9877\n",
            "Epoch 372/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1732 - accuracy: 0.9364 - val_loss: 0.0312 - val_accuracy: 0.9969\n",
            "Epoch 373/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1616 - accuracy: 0.9406 - val_loss: 0.0360 - val_accuracy: 0.9918\n",
            "Epoch 374/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.2072 - accuracy: 0.9239 - val_loss: 0.0435 - val_accuracy: 0.9908\n",
            "Epoch 375/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1752 - accuracy: 0.9347 - val_loss: 0.0382 - val_accuracy: 0.9877\n",
            "Epoch 376/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1642 - accuracy: 0.9408 - val_loss: 0.0336 - val_accuracy: 0.9908\n",
            "Epoch 377/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1581 - accuracy: 0.9437 - val_loss: 0.0363 - val_accuracy: 0.9928\n",
            "Epoch 378/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1805 - accuracy: 0.9327 - val_loss: 0.0353 - val_accuracy: 0.9918\n",
            "Epoch 379/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1566 - accuracy: 0.9404 - val_loss: 0.0309 - val_accuracy: 0.9938\n",
            "Epoch 380/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1705 - accuracy: 0.9389 - val_loss: 0.0585 - val_accuracy: 0.9820\n",
            "Epoch 381/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1610 - accuracy: 0.9415 - val_loss: 0.5905 - val_accuracy: 0.8286\n",
            "Epoch 382/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1843 - accuracy: 0.9312 - val_loss: 0.0389 - val_accuracy: 0.9928\n",
            "Epoch 383/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1602 - accuracy: 0.9402 - val_loss: 0.0384 - val_accuracy: 0.9913\n",
            "Epoch 384/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1600 - accuracy: 0.9435 - val_loss: 0.0293 - val_accuracy: 0.9944\n",
            "Epoch 385/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1596 - accuracy: 0.9428 - val_loss: 0.0348 - val_accuracy: 0.9923\n",
            "Epoch 386/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1503 - accuracy: 0.9463 - val_loss: 0.0350 - val_accuracy: 0.9928\n",
            "Epoch 387/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1723 - accuracy: 0.9364 - val_loss: 0.0481 - val_accuracy: 0.9872\n",
            "Epoch 388/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1401 - accuracy: 0.9481 - val_loss: 0.0343 - val_accuracy: 0.9918\n",
            "Epoch 389/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1453 - accuracy: 0.9452 - val_loss: 0.0384 - val_accuracy: 0.9887\n",
            "Epoch 390/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2577 - accuracy: 0.9063 - val_loss: 0.0484 - val_accuracy: 0.9897\n",
            "Epoch 391/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1680 - accuracy: 0.9433 - val_loss: 0.0416 - val_accuracy: 0.9887\n",
            "Epoch 392/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1778 - accuracy: 0.9334 - val_loss: 0.0425 - val_accuracy: 0.9887\n",
            "Epoch 393/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.2391 - accuracy: 0.9169 - val_loss: 0.0428 - val_accuracy: 0.9913\n",
            "Epoch 394/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1696 - accuracy: 0.9331 - val_loss: 0.0470 - val_accuracy: 0.9892\n",
            "Epoch 395/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1681 - accuracy: 0.9362 - val_loss: 0.0414 - val_accuracy: 0.9944\n",
            "Epoch 396/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1492 - accuracy: 0.9459 - val_loss: 0.0440 - val_accuracy: 0.9913\n",
            "Epoch 397/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.1486 - accuracy: 0.9468 - val_loss: 0.0302 - val_accuracy: 0.9944\n",
            "Epoch 398/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1703 - accuracy: 0.9353 - val_loss: 0.0315 - val_accuracy: 0.9938\n",
            "Epoch 399/2000\n",
            "4547/4547 [==============================] - 2s 347us/step - loss: 0.1754 - accuracy: 0.9375 - val_loss: 0.0339 - val_accuracy: 0.9944\n",
            "Epoch 400/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 0.1481 - accuracy: 0.9485 - val_loss: 0.0273 - val_accuracy: 0.9944\n",
            "Epoch 401/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1259 - accuracy: 0.9547 - val_loss: 0.0248 - val_accuracy: 0.9949\n",
            "Epoch 402/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1279 - accuracy: 0.9558 - val_loss: 0.0327 - val_accuracy: 0.9923\n",
            "Epoch 403/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1430 - accuracy: 0.9483 - val_loss: 0.0380 - val_accuracy: 0.9908\n",
            "Epoch 404/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1716 - accuracy: 0.9378 - val_loss: 0.0385 - val_accuracy: 0.9918\n",
            "Epoch 405/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1577 - accuracy: 0.9439 - val_loss: 0.0286 - val_accuracy: 0.9949\n",
            "Epoch 406/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1369 - accuracy: 0.9512 - val_loss: 0.0291 - val_accuracy: 0.9928\n",
            "Epoch 407/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1392 - accuracy: 0.9529 - val_loss: 0.0279 - val_accuracy: 0.9908\n",
            "Epoch 408/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1299 - accuracy: 0.9538 - val_loss: 0.0231 - val_accuracy: 0.9949\n",
            "Epoch 409/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1233 - accuracy: 0.9562 - val_loss: 0.0236 - val_accuracy: 0.9944\n",
            "Epoch 410/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1210 - accuracy: 0.9558 - val_loss: 0.0265 - val_accuracy: 0.9923\n",
            "Epoch 411/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1479 - accuracy: 0.9474 - val_loss: 0.0277 - val_accuracy: 0.9918\n",
            "Epoch 412/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1265 - accuracy: 0.9556 - val_loss: 0.0280 - val_accuracy: 0.9938\n",
            "Epoch 413/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1411 - accuracy: 0.9459 - val_loss: 0.0407 - val_accuracy: 0.9882\n",
            "Epoch 414/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1362 - accuracy: 0.9529 - val_loss: 0.0340 - val_accuracy: 0.9897\n",
            "Epoch 415/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1584 - accuracy: 0.9417 - val_loss: 0.0406 - val_accuracy: 0.9887\n",
            "Epoch 416/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1308 - accuracy: 0.9534 - val_loss: 0.0259 - val_accuracy: 0.9938\n",
            "Epoch 417/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1402 - accuracy: 0.9492 - val_loss: 0.0284 - val_accuracy: 0.9938\n",
            "Epoch 418/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1348 - accuracy: 0.9534 - val_loss: 0.0318 - val_accuracy: 0.9923\n",
            "Epoch 419/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2356 - accuracy: 0.9166 - val_loss: 0.0850 - val_accuracy: 0.9702\n",
            "Epoch 420/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2098 - accuracy: 0.9235 - val_loss: 0.0427 - val_accuracy: 0.9887\n",
            "Epoch 421/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1620 - accuracy: 0.9411 - val_loss: 0.0379 - val_accuracy: 0.9913\n",
            "Epoch 422/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1487 - accuracy: 0.9448 - val_loss: 0.0334 - val_accuracy: 0.9913\n",
            "Epoch 423/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1494 - accuracy: 0.9466 - val_loss: 0.0352 - val_accuracy: 0.9913\n",
            "Epoch 424/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1737 - accuracy: 0.9373 - val_loss: 0.0319 - val_accuracy: 0.9938\n",
            "Epoch 425/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.2120 - accuracy: 0.9248 - val_loss: 0.0441 - val_accuracy: 0.9903\n",
            "Epoch 426/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.1471 - accuracy: 0.9466 - val_loss: 0.0307 - val_accuracy: 0.9918\n",
            "Epoch 427/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1722 - accuracy: 0.9360 - val_loss: 0.0294 - val_accuracy: 0.9938\n",
            "Epoch 428/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1453 - accuracy: 0.9437 - val_loss: 0.0273 - val_accuracy: 0.9933\n",
            "Epoch 429/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1334 - accuracy: 0.9525 - val_loss: 0.0287 - val_accuracy: 0.9918\n",
            "Epoch 430/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1546 - accuracy: 0.9415 - val_loss: 0.0309 - val_accuracy: 0.9923\n",
            "Epoch 431/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1267 - accuracy: 0.9560 - val_loss: 0.0280 - val_accuracy: 0.9928\n",
            "Epoch 432/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1305 - accuracy: 0.9551 - val_loss: 0.0304 - val_accuracy: 0.9913\n",
            "Epoch 433/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1385 - accuracy: 0.9516 - val_loss: 0.0310 - val_accuracy: 0.9949\n",
            "Epoch 434/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1332 - accuracy: 0.9516 - val_loss: 0.0238 - val_accuracy: 0.9944\n",
            "Epoch 435/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1471 - accuracy: 0.9477 - val_loss: 0.0282 - val_accuracy: 0.9969\n",
            "Epoch 436/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1216 - accuracy: 0.9556 - val_loss: 0.0207 - val_accuracy: 0.9954\n",
            "Epoch 437/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1292 - accuracy: 0.9527 - val_loss: 0.0362 - val_accuracy: 0.9887\n",
            "Epoch 438/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1395 - accuracy: 0.9516 - val_loss: 0.0446 - val_accuracy: 0.9841\n",
            "Epoch 439/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1391 - accuracy: 0.9494 - val_loss: 0.0272 - val_accuracy: 0.9938\n",
            "Epoch 440/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1445 - accuracy: 0.9472 - val_loss: 0.0259 - val_accuracy: 0.9964\n",
            "Epoch 441/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1512 - accuracy: 0.9437 - val_loss: 0.0322 - val_accuracy: 0.9928\n",
            "Epoch 442/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1155 - accuracy: 0.9595 - val_loss: 0.0240 - val_accuracy: 0.9938\n",
            "Epoch 443/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1276 - accuracy: 0.9547 - val_loss: 0.0255 - val_accuracy: 0.9949\n",
            "Epoch 444/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1307 - accuracy: 0.9543 - val_loss: 0.0361 - val_accuracy: 0.9908\n",
            "Epoch 445/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1828 - accuracy: 0.9340 - val_loss: 0.0365 - val_accuracy: 0.9923\n",
            "Epoch 446/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1106 - accuracy: 0.9617 - val_loss: 0.0222 - val_accuracy: 0.9959\n",
            "Epoch 447/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1327 - accuracy: 0.9514 - val_loss: 0.0370 - val_accuracy: 0.9908\n",
            "Epoch 448/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1721 - accuracy: 0.9397 - val_loss: 0.0338 - val_accuracy: 0.9933\n",
            "Epoch 449/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1386 - accuracy: 0.9507 - val_loss: 0.0251 - val_accuracy: 0.9949\n",
            "Epoch 450/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1297 - accuracy: 0.9560 - val_loss: 0.0305 - val_accuracy: 0.9908\n",
            "Epoch 451/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1539 - accuracy: 0.9446 - val_loss: 0.0265 - val_accuracy: 0.9933\n",
            "Epoch 452/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1900 - accuracy: 0.9375 - val_loss: 0.0419 - val_accuracy: 0.9861\n",
            "Epoch 453/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1765 - accuracy: 0.9349 - val_loss: 0.0282 - val_accuracy: 0.9944\n",
            "Epoch 454/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1462 - accuracy: 0.9466 - val_loss: 0.0274 - val_accuracy: 0.9938\n",
            "Epoch 455/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1270 - accuracy: 0.9549 - val_loss: 0.0269 - val_accuracy: 0.9944\n",
            "Epoch 456/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1181 - accuracy: 0.9573 - val_loss: 0.0242 - val_accuracy: 0.9949\n",
            "Epoch 457/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1595 - accuracy: 0.9433 - val_loss: 0.0311 - val_accuracy: 0.9933\n",
            "Epoch 458/2000\n",
            "4547/4547 [==============================] - 2s 345us/step - loss: 0.1258 - accuracy: 0.9554 - val_loss: 0.0217 - val_accuracy: 0.9974\n",
            "Epoch 459/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1130 - accuracy: 0.9589 - val_loss: 0.0211 - val_accuracy: 0.9964\n",
            "Epoch 460/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1226 - accuracy: 0.9576 - val_loss: 0.0207 - val_accuracy: 0.9959\n",
            "Epoch 461/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1364 - accuracy: 0.9532 - val_loss: 0.0233 - val_accuracy: 0.9949\n",
            "Epoch 462/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1390 - accuracy: 0.9507 - val_loss: 0.0300 - val_accuracy: 0.9928\n",
            "Epoch 463/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1193 - accuracy: 0.9584 - val_loss: 0.0235 - val_accuracy: 0.9959\n",
            "Epoch 464/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1164 - accuracy: 0.9567 - val_loss: 0.0207 - val_accuracy: 0.9969\n",
            "Epoch 465/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1152 - accuracy: 0.9617 - val_loss: 0.0206 - val_accuracy: 0.9944\n",
            "Epoch 466/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1216 - accuracy: 0.9567 - val_loss: 0.0216 - val_accuracy: 0.9949\n",
            "Epoch 467/2000\n",
            "4547/4547 [==============================] - 1s 328us/step - loss: 0.1557 - accuracy: 0.9463 - val_loss: 0.0270 - val_accuracy: 0.9944\n",
            "Epoch 468/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1288 - accuracy: 0.9536 - val_loss: 0.0278 - val_accuracy: 0.9954\n",
            "Epoch 469/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1350 - accuracy: 0.9521 - val_loss: 0.0255 - val_accuracy: 0.9949\n",
            "Epoch 470/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1524 - accuracy: 0.9450 - val_loss: 0.0299 - val_accuracy: 0.9913\n",
            "Epoch 471/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1310 - accuracy: 0.9536 - val_loss: 0.0224 - val_accuracy: 0.9959\n",
            "Epoch 472/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1343 - accuracy: 0.9523 - val_loss: 0.0286 - val_accuracy: 0.9923\n",
            "Epoch 473/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1214 - accuracy: 0.9587 - val_loss: 0.0347 - val_accuracy: 0.9887\n",
            "Epoch 474/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.0987 - accuracy: 0.9639 - val_loss: 0.0328 - val_accuracy: 0.9867\n",
            "Epoch 475/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1334 - accuracy: 0.9543 - val_loss: 0.0274 - val_accuracy: 0.9933\n",
            "Epoch 476/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1223 - accuracy: 0.9523 - val_loss: 0.0276 - val_accuracy: 0.9933\n",
            "Epoch 477/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1197 - accuracy: 0.9549 - val_loss: 0.0407 - val_accuracy: 0.9887\n",
            "Epoch 478/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1577 - accuracy: 0.9479 - val_loss: 0.0416 - val_accuracy: 0.9892\n",
            "Epoch 479/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.2462 - accuracy: 0.9169 - val_loss: 0.0783 - val_accuracy: 0.9733\n",
            "Epoch 480/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1595 - accuracy: 0.9433 - val_loss: 0.0251 - val_accuracy: 0.9969\n",
            "Epoch 481/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1280 - accuracy: 0.9560 - val_loss: 0.0251 - val_accuracy: 0.9954\n",
            "Epoch 482/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1405 - accuracy: 0.9503 - val_loss: 0.0253 - val_accuracy: 0.9949\n",
            "Epoch 483/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.1562 - accuracy: 0.9448 - val_loss: 0.0333 - val_accuracy: 0.9938\n",
            "Epoch 484/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1462 - accuracy: 0.9459 - val_loss: 0.0291 - val_accuracy: 0.9949\n",
            "Epoch 485/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1231 - accuracy: 0.9551 - val_loss: 0.0270 - val_accuracy: 0.9933\n",
            "Epoch 486/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1462 - accuracy: 0.9488 - val_loss: 0.0247 - val_accuracy: 0.9928\n",
            "Epoch 487/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1072 - accuracy: 0.9635 - val_loss: 0.0208 - val_accuracy: 0.9944\n",
            "Epoch 488/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1507 - accuracy: 0.9488 - val_loss: 0.0285 - val_accuracy: 0.9928\n",
            "Epoch 489/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1263 - accuracy: 0.9532 - val_loss: 0.0239 - val_accuracy: 0.9954\n",
            "Epoch 490/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2119 - accuracy: 0.9206 - val_loss: 0.0317 - val_accuracy: 0.9938\n",
            "Epoch 491/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1373 - accuracy: 0.9521 - val_loss: 0.0251 - val_accuracy: 0.9938\n",
            "Epoch 492/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1312 - accuracy: 0.9549 - val_loss: 0.0188 - val_accuracy: 0.9974\n",
            "Epoch 493/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1780 - accuracy: 0.9367 - val_loss: 0.0291 - val_accuracy: 0.9938\n",
            "Epoch 494/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1706 - accuracy: 0.9382 - val_loss: 0.0296 - val_accuracy: 0.9938\n",
            "Epoch 495/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1292 - accuracy: 0.9538 - val_loss: 0.0199 - val_accuracy: 0.9964\n",
            "Epoch 496/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1348 - accuracy: 0.9540 - val_loss: 0.0188 - val_accuracy: 0.9964\n",
            "Epoch 497/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1357 - accuracy: 0.9547 - val_loss: 0.0204 - val_accuracy: 0.9969\n",
            "Epoch 498/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1189 - accuracy: 0.9582 - val_loss: 0.0173 - val_accuracy: 0.9974\n",
            "Epoch 499/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1248 - accuracy: 0.9543 - val_loss: 0.0190 - val_accuracy: 0.9964\n",
            "Epoch 500/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1117 - accuracy: 0.9611 - val_loss: 0.0182 - val_accuracy: 0.9979\n",
            "Epoch 501/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.0971 - accuracy: 0.9688 - val_loss: 0.0181 - val_accuracy: 0.9944\n",
            "Epoch 502/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1131 - accuracy: 0.9580 - val_loss: 0.0233 - val_accuracy: 0.9933\n",
            "Epoch 503/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1918 - accuracy: 0.9327 - val_loss: 0.0392 - val_accuracy: 0.9913\n",
            "Epoch 504/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1963 - accuracy: 0.9307 - val_loss: 0.0284 - val_accuracy: 0.9923\n",
            "Epoch 505/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1389 - accuracy: 0.9521 - val_loss: 0.0276 - val_accuracy: 0.9944\n",
            "Epoch 506/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1295 - accuracy: 0.9514 - val_loss: 0.0214 - val_accuracy: 0.9938\n",
            "Epoch 507/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1175 - accuracy: 0.9593 - val_loss: 0.0228 - val_accuracy: 0.9959\n",
            "Epoch 508/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1444 - accuracy: 0.9490 - val_loss: 0.0231 - val_accuracy: 0.9933\n",
            "Epoch 509/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1259 - accuracy: 0.9540 - val_loss: 0.0225 - val_accuracy: 0.9954\n",
            "Epoch 510/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1134 - accuracy: 0.9620 - val_loss: 0.0220 - val_accuracy: 0.9949\n",
            "Epoch 511/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.2450 - accuracy: 0.9094 - val_loss: 0.0542 - val_accuracy: 0.9903\n",
            "Epoch 512/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1443 - accuracy: 0.9496 - val_loss: 0.0264 - val_accuracy: 0.9928\n",
            "Epoch 513/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.2047 - accuracy: 0.9274 - val_loss: 0.0416 - val_accuracy: 0.9903\n",
            "Epoch 514/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1313 - accuracy: 0.9523 - val_loss: 0.0221 - val_accuracy: 0.9959\n",
            "Epoch 515/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1420 - accuracy: 0.9514 - val_loss: 0.0306 - val_accuracy: 0.9938\n",
            "Epoch 516/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1204 - accuracy: 0.9547 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
            "Epoch 517/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1561 - accuracy: 0.9422 - val_loss: 0.0245 - val_accuracy: 0.9938\n",
            "Epoch 518/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1208 - accuracy: 0.9582 - val_loss: 0.0189 - val_accuracy: 0.9954\n",
            "Epoch 519/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1223 - accuracy: 0.9560 - val_loss: 0.0188 - val_accuracy: 0.9954\n",
            "Epoch 520/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1034 - accuracy: 0.9635 - val_loss: 0.0152 - val_accuracy: 0.9959\n",
            "Epoch 521/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1366 - accuracy: 0.9510 - val_loss: 0.0198 - val_accuracy: 0.9969\n",
            "Epoch 522/2000\n",
            "4547/4547 [==============================] - 2s 343us/step - loss: 0.0990 - accuracy: 0.9648 - val_loss: 0.0163 - val_accuracy: 0.9969\n",
            "Epoch 523/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1317 - accuracy: 0.9527 - val_loss: 0.0182 - val_accuracy: 0.9969\n",
            "Epoch 524/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1051 - accuracy: 0.9611 - val_loss: 0.0225 - val_accuracy: 0.9928\n",
            "Epoch 525/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.0194 - val_accuracy: 0.9933\n",
            "Epoch 526/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1112 - accuracy: 0.9613 - val_loss: 0.0215 - val_accuracy: 0.9954\n",
            "Epoch 527/2000\n",
            "4547/4547 [==============================] - 2s 330us/step - loss: 0.1384 - accuracy: 0.9505 - val_loss: 0.0211 - val_accuracy: 0.9959\n",
            "Epoch 528/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.0995 - accuracy: 0.9620 - val_loss: 0.0202 - val_accuracy: 0.9964\n",
            "Epoch 529/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2109 - accuracy: 0.9250 - val_loss: 0.0547 - val_accuracy: 0.9867\n",
            "Epoch 530/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1534 - accuracy: 0.9485 - val_loss: 0.0272 - val_accuracy: 0.9954\n",
            "Epoch 531/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.2459 - accuracy: 0.9112 - val_loss: 0.0402 - val_accuracy: 0.9928\n",
            "Epoch 532/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1407 - accuracy: 0.9512 - val_loss: 0.0234 - val_accuracy: 0.9954\n",
            "Epoch 533/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1135 - accuracy: 0.9571 - val_loss: 0.0163 - val_accuracy: 0.9954\n",
            "Epoch 534/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1196 - accuracy: 0.9582 - val_loss: 0.0232 - val_accuracy: 0.9944\n",
            "Epoch 535/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.1071 - accuracy: 0.9582 - val_loss: 0.0137 - val_accuracy: 0.9990\n",
            "Epoch 536/2000\n",
            "4547/4547 [==============================] - 2s 344us/step - loss: 0.1196 - accuracy: 0.9573 - val_loss: 0.0161 - val_accuracy: 0.9959\n",
            "Epoch 537/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1048 - accuracy: 0.9606 - val_loss: 0.0162 - val_accuracy: 0.9959\n",
            "Epoch 538/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1022 - accuracy: 0.9613 - val_loss: 0.0161 - val_accuracy: 0.9979\n",
            "Epoch 539/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1115 - accuracy: 0.9591 - val_loss: 0.0227 - val_accuracy: 0.9938\n",
            "Epoch 540/2000\n",
            "4547/4547 [==============================] - 2s 333us/step - loss: 0.1144 - accuracy: 0.9584 - val_loss: 0.0182 - val_accuracy: 0.9969\n",
            "Epoch 541/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1797 - accuracy: 0.9340 - val_loss: 0.0302 - val_accuracy: 0.9949\n",
            "Epoch 542/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1241 - accuracy: 0.9558 - val_loss: 0.0196 - val_accuracy: 0.9959\n",
            "Epoch 543/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1197 - accuracy: 0.9576 - val_loss: 0.0196 - val_accuracy: 0.9969\n",
            "Epoch 544/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.1595 - accuracy: 0.9474 - val_loss: 0.0184 - val_accuracy: 0.9974\n",
            "Epoch 545/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1290 - accuracy: 0.9576 - val_loss: 0.0192 - val_accuracy: 0.9944\n",
            "Epoch 546/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1197 - accuracy: 0.9573 - val_loss: 0.0169 - val_accuracy: 0.9969\n",
            "Epoch 547/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.0986 - accuracy: 0.9659 - val_loss: 0.0174 - val_accuracy: 0.9959\n",
            "Epoch 548/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1089 - accuracy: 0.9598 - val_loss: 0.0199 - val_accuracy: 0.9969\n",
            "Epoch 549/2000\n",
            "4547/4547 [==============================] - 2s 331us/step - loss: 0.1138 - accuracy: 0.9589 - val_loss: 0.0170 - val_accuracy: 0.9969\n",
            "Epoch 550/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1152 - accuracy: 0.9595 - val_loss: 0.0195 - val_accuracy: 0.9964\n",
            "Epoch 551/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1085 - accuracy: 0.9648 - val_loss: 0.0177 - val_accuracy: 0.9964\n",
            "Epoch 552/2000\n",
            "4547/4547 [==============================] - 2s 332us/step - loss: 0.0934 - accuracy: 0.9681 - val_loss: 0.0165 - val_accuracy: 0.9959\n",
            "Epoch 553/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1055 - accuracy: 0.9624 - val_loss: 0.0228 - val_accuracy: 0.9949\n",
            "Epoch 554/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1088 - accuracy: 0.9626 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
            "Epoch 555/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1092 - accuracy: 0.9626 - val_loss: 0.0190 - val_accuracy: 0.9959\n",
            "Epoch 556/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.2365 - accuracy: 0.9164 - val_loss: 0.0652 - val_accuracy: 0.9820\n",
            "Epoch 557/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1596 - accuracy: 0.9430 - val_loss: 0.0272 - val_accuracy: 0.9944\n",
            "Epoch 558/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1218 - accuracy: 0.9534 - val_loss: 0.0220 - val_accuracy: 0.9949\n",
            "Epoch 559/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.1061 - accuracy: 0.9620 - val_loss: 0.0164 - val_accuracy: 0.9969\n",
            "Epoch 560/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1088 - accuracy: 0.9628 - val_loss: 0.0154 - val_accuracy: 0.9964\n",
            "Epoch 561/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1209 - accuracy: 0.9576 - val_loss: 0.0198 - val_accuracy: 0.9954\n",
            "Epoch 562/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1041 - accuracy: 0.9611 - val_loss: 0.0205 - val_accuracy: 0.9944\n",
            "Epoch 563/2000\n",
            "4547/4547 [==============================] - 2s 341us/step - loss: 0.1507 - accuracy: 0.9470 - val_loss: 0.0196 - val_accuracy: 0.9944\n",
            "Epoch 564/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.0996 - accuracy: 0.9653 - val_loss: 0.0151 - val_accuracy: 0.9959\n",
            "Epoch 565/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1050 - accuracy: 0.9637 - val_loss: 0.0191 - val_accuracy: 0.9949\n",
            "Epoch 566/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.0974 - accuracy: 0.9653 - val_loss: 0.0210 - val_accuracy: 0.9938\n",
            "Epoch 567/2000\n",
            "4547/4547 [==============================] - 2s 354us/step - loss: 0.1447 - accuracy: 0.9540 - val_loss: 0.0210 - val_accuracy: 0.9964\n",
            "Epoch 568/2000\n",
            "4547/4547 [==============================] - 2s 349us/step - loss: 0.1337 - accuracy: 0.9521 - val_loss: 0.0281 - val_accuracy: 0.9933\n",
            "Epoch 569/2000\n",
            "4547/4547 [==============================] - 2s 348us/step - loss: 0.1158 - accuracy: 0.9589 - val_loss: 0.0190 - val_accuracy: 0.9949\n",
            "Epoch 570/2000\n",
            "4547/4547 [==============================] - 2s 355us/step - loss: 0.0917 - accuracy: 0.9683 - val_loss: 0.0176 - val_accuracy: 0.9959\n",
            "Epoch 571/2000\n",
            "4547/4547 [==============================] - 2s 351us/step - loss: 0.1183 - accuracy: 0.9593 - val_loss: 0.0189 - val_accuracy: 0.9949\n",
            "Epoch 572/2000\n",
            "4547/4547 [==============================] - 2s 348us/step - loss: 0.1386 - accuracy: 0.9516 - val_loss: 0.0225 - val_accuracy: 0.9954\n",
            "Epoch 573/2000\n",
            "4547/4547 [==============================] - 2s 348us/step - loss: 0.1086 - accuracy: 0.9589 - val_loss: 0.0167 - val_accuracy: 0.9964\n",
            "Epoch 574/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.0936 - accuracy: 0.9675 - val_loss: 0.0196 - val_accuracy: 0.9954\n",
            "Epoch 575/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1078 - accuracy: 0.9611 - val_loss: 0.0183 - val_accuracy: 0.9959\n",
            "Epoch 576/2000\n",
            "4547/4547 [==============================] - 2s 340us/step - loss: 0.1010 - accuracy: 0.9655 - val_loss: 0.0161 - val_accuracy: 0.9969\n",
            "Epoch 577/2000\n",
            "4547/4547 [==============================] - 2s 342us/step - loss: 0.1029 - accuracy: 0.9628 - val_loss: 0.0255 - val_accuracy: 0.9954\n",
            "Epoch 578/2000\n",
            "4547/4547 [==============================] - 2s 337us/step - loss: 0.1095 - accuracy: 0.9578 - val_loss: 0.0212 - val_accuracy: 0.9938\n",
            "Epoch 579/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1089 - accuracy: 0.9604 - val_loss: 0.0240 - val_accuracy: 0.9938\n",
            "Epoch 580/2000\n",
            "4547/4547 [==============================] - 2s 336us/step - loss: 0.1086 - accuracy: 0.9609 - val_loss: 0.0238 - val_accuracy: 0.9964\n",
            "Epoch 581/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1115 - accuracy: 0.9611 - val_loss: 0.0212 - val_accuracy: 0.9938\n",
            "Epoch 582/2000\n",
            "4547/4547 [==============================] - 2s 339us/step - loss: 0.1057 - accuracy: 0.9637 - val_loss: 0.0180 - val_accuracy: 0.9974\n",
            "Epoch 583/2000\n",
            "4547/4547 [==============================] - 2s 338us/step - loss: 0.1012 - accuracy: 0.9644 - val_loss: 0.0186 - val_accuracy: 0.9964\n",
            "Epoch 584/2000\n",
            "4547/4547 [==============================] - 2s 334us/step - loss: 0.1142 - accuracy: 0.9595 - val_loss: 0.0286 - val_accuracy: 0.9908\n",
            "Epoch 585/2000\n",
            "4547/4547 [==============================] - 2s 335us/step - loss: 0.0991 - accuracy: 0.9644 - val_loss: 0.0213 - val_accuracy: 0.9944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztP2KGPxzWzE",
        "colab_type": "code",
        "outputId": "020afb46-e4d8-486d-f466-cd049c335f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "####################################\n",
        "#트레이닝 epoch에 따라 loss와 accuracy 의 변화를 그래프로 시각화\n",
        "\n",
        "\n",
        "fig , loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(history.history['loss'], 'r', label = 'train loss')\n",
        "\n",
        "acc_ax.plot(history.history['accuracy'], 'b', label = 'train acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='lower left')\n",
        "acc_ax.legend(loc = 'upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUqQxpHEzdcg",
        "colab_type": "code",
        "outputId": "1ae9886a-0d4b-4b9c-efa4-a629442140cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_accuracy = model3.evaluate(x_test3, y_test3)\n",
        "print(\"test accuracy: \",test_accuracy,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1950/1950 [==============================] - 0s 75us/step\n",
            "test accuracy:  0.6353846192359924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud9658JczNq0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
